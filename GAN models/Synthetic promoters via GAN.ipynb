{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ-NN1Q1TkoH"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGlUAdsVsVSB",
    "outputId": "5912a049-76b1-4670-f1c7-b150ab3578d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Biopython\n",
      "  Downloading biopython-1.79-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from Biopython) (1.19.2)\n",
      "Installing collected packages: Biopython\n",
      "Successfully installed Biopython-1.79\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4-po12cGVwN",
    "outputId": "8e3a88a9-164c-440a-cbbe-463b77d81525"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-04-25 13:08:14--  https://mafft.cbrc.jp/alignment/software/mafft_7.505-1_amd64.deb\n",
      "Resolving mafft.cbrc.jp (mafft.cbrc.jp)... 133.1.49.95\n",
      "Connecting to mafft.cbrc.jp (mafft.cbrc.jp)|133.1.49.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3661204 (3.5M)\n",
      "Saving to: 'mafft_7.505-1_amd64.deb.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 74.2K 48s\n",
      "    50K .......... .......... .......... .......... ..........  2%  152K 35s\n",
      "   100K .......... .......... .......... .......... ..........  4% 1.71M 24s\n",
      "   150K .......... .......... .......... .......... ..........  5%  156K 23s\n",
      "   200K .......... .......... .......... .......... ..........  6%  154K 22s\n",
      "   250K .......... .......... .......... .......... ..........  8%  158K 22s\n",
      "   300K .......... .......... .......... .......... ..........  9% 3.13M 19s\n",
      "   350K .......... .......... .......... .......... .......... 11% 3.09M 16s\n",
      "   400K .......... .......... .......... .......... .......... 12% 3.00M 14s\n",
      "   450K .......... .......... .......... .......... .......... 13%  176K 14s\n",
      "   500K .......... .......... .......... .......... .......... 15% 2.94M 13s\n",
      "   550K .......... .......... .......... .......... .......... 16% 4.21M 12s\n",
      "   600K .......... .......... .......... .......... .......... 18%  129K 12s\n",
      "   650K .......... .......... .......... .......... .......... 19% 3.79M 11s\n",
      "   700K .......... .......... .......... .......... .......... 20% 4.53M 10s\n",
      "   750K .......... .......... .......... .......... .......... 22% 6.17M 10s\n",
      "   800K .......... .......... .......... .......... .......... 23% 8.86M 9s\n",
      "   850K .......... .......... .......... .......... .......... 25% 19.9M 8s\n",
      "   900K .......... .......... .......... .......... .......... 26% 34.6M 8s\n",
      "   950K .......... .......... .......... .......... .......... 27% 39.4M 7s\n",
      "  1000K .......... .......... .......... .......... .......... 29% 25.9M 7s\n",
      "  1050K .......... .......... .......... .......... .......... 30% 35.6M 6s\n",
      "  1100K .......... .......... .......... .......... .......... 32% 41.7M 6s\n",
      "  1150K .......... .......... .......... .......... .......... 33% 23.2M 6s\n",
      "  1200K .......... .......... .......... .......... .......... 34% 32.5M 5s\n",
      "  1250K .......... .......... .......... .......... .......... 36%  264K 5s\n",
      "  1300K .......... .......... .......... .......... .......... 37%  126K 6s\n",
      "  1350K .......... .......... .......... .......... .......... 39% 3.63M 5s\n",
      "  1400K .......... .......... .......... .......... .......... 40% 5.61M 5s\n",
      "  1450K .......... .......... .......... .......... .......... 41% 1.34M 5s\n",
      "  1500K .......... .......... .......... .......... .......... 43% 3.87M 5s\n",
      "  1550K .......... .......... .......... .......... .......... 44% 4.79M 4s\n",
      "  1600K .......... .......... .......... .......... .......... 46% 4.70M 4s\n",
      "  1650K .......... .......... .......... .......... .......... 47% 4.55M 4s\n",
      "  1700K .......... .......... .......... .......... .......... 48% 8.09M 4s\n",
      "  1750K .......... .......... .......... .......... .......... 50% 7.62M 3s\n",
      "  1800K .......... .......... .......... .......... .......... 51% 10.7M 3s\n",
      "  1850K .......... .......... .......... .......... .......... 53%  483K 3s\n",
      "  1900K .......... .......... .......... .......... .......... 54%  953K 3s\n",
      "  1950K .......... .......... .......... .......... .......... 55%  792K 3s\n",
      "  2000K .......... .......... .......... .......... .......... 57% 3.48M 3s\n",
      "  2050K .......... .......... .......... .......... .......... 58%  949K 3s\n",
      "  2100K .......... .......... .......... .......... .......... 60% 3.40M 3s\n",
      "  2150K .......... .......... .......... .......... .......... 61%  276K 2s\n",
      "  2200K .......... .......... .......... .......... .......... 62%  797K 2s\n",
      "  2250K .......... .......... .......... .......... .......... 64% 3.39M 2s\n",
      "  2300K .......... .......... .......... .......... .......... 65%  834K 2s\n",
      "  2350K .......... .......... .......... .......... .......... 67% 3.09M 2s\n",
      "  2400K .......... .......... .......... .......... .......... 68% 86.7K 2s\n",
      "  2450K .......... .......... .......... .......... .......... 69% 3.40M 2s\n",
      "  2500K .......... .......... .......... .......... .......... 71% 3.41M 2s\n",
      "  2550K .......... .......... .......... .......... .......... 72% 3.84M 2s\n",
      "  2600K .......... .......... .......... .......... .......... 74% 3.23M 2s\n",
      "  2650K .......... .......... .......... .......... .......... 75% 3.29M 2s\n",
      "  2700K .......... .......... .......... .......... .......... 76% 3.54M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 78% 3.27M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 79% 3.56M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 81%  234K 1s\n",
      "  2900K .......... .......... .......... .......... .......... 82% 2.84M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 83% 2.53M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 85% 1.11M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 86%  205K 1s\n",
      "  3100K .......... .......... .......... .......... .......... 88% 2.28M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 89% 3.03M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 90%  901K 1s\n",
      "  3250K .......... .......... .......... .......... .......... 92%  452K 0s\n",
      "  3300K .......... .......... .......... .......... .......... 93%  261K 0s\n",
      "  3350K .......... .......... .......... .......... .......... 95% 3.77M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 96% 3.02M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 97% 3.78M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 99%  178K 0s\n",
      "  3550K .......... .......... .....                           100% 3.12M=6.1s\n",
      "\n",
      "2023-04-25 13:08:22 (587 KB/s) - 'mafft_7.505-1_amd64.deb.1' saved [3661204/3661204]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://mafft.cbrc.jp/alignment/software/mafft_7.505-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ1CNIbIGY89",
    "outputId": "bb98d1be-0d92-46ce-87b9-992d06e6f2b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo dpkg -i mafft_7.505-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0TtZDBSGkhK",
    "outputId": "9bbdc2e8-42f7-4b64-f5d7-cc79adb8d841"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!which mafft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uatqDR2jPOnt",
    "outputId": "b2f13ecb-acd2-4f24-c465-9b6277fa1302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasta_one_hot_encoder in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from fasta_one_hot_encoder) (1.19.2)\n",
      "Requirement already satisfied: sklearn in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from fasta_one_hot_encoder) (0.0.post1)\n",
      "Requirement already satisfied: pandas in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from fasta_one_hot_encoder) (1.1.5)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from fasta_one_hot_encoder) (4.64.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from pandas->fasta_one_hot_encoder) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from pandas->fasta_one_hot_encoder) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->fasta_one_hot_encoder) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from tqdm->fasta_one_hot_encoder) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from importlib-resources->tqdm->fasta_one_hot_encoder) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasta_one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yS5m81AZyu8F",
    "outputId": "dbcd80a2-302f-4c3b-9498-e5eac2fb3f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: gdown: command not found\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1Ou-VgT696SR7-fdS9o788WMTUX4PGQ-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-win_amd64.whl (167.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[K     |████████████████████████████████| 499 kB 103.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.5 pytz-2022.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 99.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from matplotlib) (1.19.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from matplotlib) (8.3.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.3.1 matplotlib-3.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.3 MB 16 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 103.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 102.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 80.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from tensorflow) (0.37.1)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 98.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from tensorflow) (1.19.2)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 92.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 96.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 94.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 107.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 106.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 109.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 105.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (58.0.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.9 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 106.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 111.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 212 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 110.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in ./miniconda3/envs/torch_py36/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=46d0004f0c9d23dfc1b2e0ba920dd799111e07279ac7a65bd370b21855acaa1c\n",
      "  Stored in directory: /home/tmost089/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=161ae1c91b0422faf0a7ae6c309bec2b30367548b049c7694b0d9de81c8ca1e4\n",
      "  Stored in directory: /home/tmost089/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=75920 sha256=79d09733242e045db280c640c254bb9cbc945448c75d21a038b0e16b1b4bca42\n",
      "  Stored in directory: /home/tmost089/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 charset-normalizer-2.0.12 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 idna-3.4 importlib-metadata-4.8.3 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.7 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.2 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.13 werkzeug-2.0.3 wrapt-1.12.1 zipp-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.20.9-cp39-cp39-win_amd64.whl (101 kB)\n",
      "     ------------------------------------ 101.3/101.3 kB 728.8 kB/s eta 0:00:00\n",
      "Collecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.20.9 rapidfuzz-2.13.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1WPwUIw9hVg"
   },
   "source": [
    "# Importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PwF-skc8GqxD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EGYPT_LAPTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\Bio\\pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re, glob\n",
    "\n",
    "# Biological libraries\n",
    "from Bio import AlignIO\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Align.Applications import MafftCommandline\n",
    "from io import StringIO\n",
    "from Bio import AlignIO\n",
    "from Bio.pairwise2 import format_alignment \n",
    "from Bio import SeqIO\n",
    "from Bio import Align\n",
    "from Bio import pairwise2\n",
    "import Levenshtein\n",
    "\n",
    "# Pytorch libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "torch.cuda.is_available()\n",
    "\n",
    "#selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maffit(in_file,out_file):\n",
    "    mafft_exe = \"/usr/bin/mafft\"\n",
    "    mafft_cline = MafftCommandline(mafft_exe, input=in_file)\n",
    "    stdout, stderr = mafft_cline()\n",
    "    with open(out_file, \"w\") as handle:\n",
    "        handle.write(stdout)\n",
    "    align = AlignIO.read(out_file, \"fasta\")\n",
    "    return align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drosophilia_alignment = Maffit('./drosophila melanogaster promoters.FASTA','drosophilia_alignment')\n",
    "drosophilia_alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(G, T, T, T, G, G, G, G, G, G, C, A, C, G, G, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(C, T, G, T, G, T, G, T, A, A, G, T, G, G, G, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(A, C, C, A, G, T, T, A, T, T, C, T, C, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(G, C, C, A, G, C, T, G, A, T, T, G, G, C, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(T, T, T, T, C, C, T, T, T, T, T, T, T, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16967</th>\n",
       "      <td>(C, G, G, T, A, G, C, G, G, A, T, G, C, T, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16968</th>\n",
       "      <td>(A, A, A, A, C, T, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16969</th>\n",
       "      <td>(T, T, G, C, A, A, C, G, A, G, A, A, T, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16970</th>\n",
       "      <td>(C, G, G, T, C, T, T, A, G, C, T, T, T, T, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16971</th>\n",
       "      <td>(G, T, A, T, T, T, C, T, C, T, A, C, G, T, T, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16972 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequences\n",
       "0      (G, T, T, T, G, G, G, G, G, G, C, A, C, G, G, ...\n",
       "1      (C, T, G, T, G, T, G, T, A, A, G, T, G, G, G, ...\n",
       "2      (A, C, C, A, G, T, T, A, T, T, C, T, C, A, A, ...\n",
       "3      (G, C, C, A, G, C, T, G, A, T, T, G, G, C, C, ...\n",
       "4      (T, T, T, T, C, C, T, T, T, T, T, T, T, T, T, ...\n",
       "...                                                  ...\n",
       "16967  (C, G, G, T, A, G, C, G, G, A, T, G, C, T, A, ...\n",
       "16968  (A, A, A, A, C, T, A, A, A, A, A, A, A, A, A, ...\n",
       "16969  (T, T, G, C, A, A, C, G, A, G, A, A, T, T, T, ...\n",
       "16970  (C, G, G, T, C, T, T, A, G, C, T, T, T, T, C, ...\n",
       "16971  (G, T, A, T, T, T, C, T, C, T, A, C, G, T, T, ...\n",
       "\n",
       "[16972 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parse the sequences\n",
    "sequences = []\n",
    "for record in SeqIO.parse('./drosophila melanogaster promoters.FASTA', \"fasta\"):\n",
    "    sequences.append(record.seq)\n",
    "# convert to dataframe\n",
    "Promoters = pd.DataFrame([sequences]).T\n",
    "Promoters.rename(columns = {0:'sequences'}, inplace = True)\n",
    "Promoters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(Promoters):\n",
    "    # create lists \n",
    "    seqs = []\n",
    "    # check length\n",
    "    max_len = len(Promoters.sequences.max())\n",
    "    for i in range(len(Promoters)):\n",
    "        check = len(Promoters.sequences[i])\n",
    "        if check < max_len:\n",
    "           Promoters.sequences[i] = pad_sequences(Promoters.sequences[i],maxlen=max_len)\n",
    "    # perform one hot encoder\n",
    "        mapping = dict(zip(\"ACGT\",range(4)))\n",
    "        seq = [mapping[m] for m in Promoters.sequences[i] if m !='N']\n",
    "        arr =  to_categorical(seq,num_classes = 4, dtype = 'int32')\n",
    "        seqs.append(arr)\n",
    "    # check length after one hot encoder\n",
    "    for j in range(len(seqs)):\n",
    "        check = len(seqs[j])\n",
    "        if check < max_len:\n",
    "          s = pad_sequences(seqs[j].T,maxlen=max_len)\n",
    "          seqs[j]=s.T\n",
    "\n",
    "    # convert numpy to tensors\n",
    "    seq_tensor = [torch.from_numpy(seq) for seq in seqs]\n",
    "    training_data  = torch.stack(seq_tensor)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16972, 600, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = preprocessing(Promoters)\n",
    "training_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKfKpu9WvX1e"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h7OomzWfvWro"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,noise_dim,gen_features,SelfAttentionLayer):\n",
    "        super(Generator,self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(noise_dim      ,gen_features , kernel_size = 4, stride=2, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features  ,gen_features  , kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features  ,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            #SelfAttentionLayer(gen_features*2,gen_features*2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            #SelfAttentionLayer(gen_features*2,gen_features*2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*3, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(gen_features*3        ,gen_features*3, kernel_size = 4, stride=1, padding = 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(gen_features*3         ,gen_features*3, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        return self.gen(x.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLJQAyMAl2Vn"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OOnLWEZkE123"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,seq_len,disc_features,SelfAttentionLayer):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(seq_len         , disc_features   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features   , disc_features   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features   , disc_features   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            #SelfAttentionLayer(disc_features,disc_features),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features   , disc_features*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features*2 , disc_features*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features*2 , disc_features*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            #SelfAttentionLayer(disc_features*2,disc_features*2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features*2 , disc_features*4 , kernel_size = 3, stride = 2, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            # nn.Conv1d(disc_features*4 , disc_features*4 , kernel_size = 3, stride = 2, padding = 0),torch.nn.InstanceNorm1d(disc_features*4,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(disc_features*4 , 1               , kernel_size = 3, stride = 2, padding = 1)),nn.Sigmoid())\n",
    "    def forward(self,x):\n",
    "        return self.disc(x.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = nn.Sequential(\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(600         , 1200   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200   , 1200   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200   , 1200   , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200   , 1200*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200*2 , 1200*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200*2 , 1200*2 , kernel_size = 3, stride = 1, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200*2 , 1200*4 , kernel_size = 3, stride = 2, padding = 1)),nn.LeakyReLU(0.2),\n",
    "            # nn.Conv1d(1200*4 , 1200*4 , kernel_size = 3, stride = 2, padding = 0),torch.nn.InstanceNorm1d(1200*4,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(1200*4 , 1               , kernel_size = 3, stride = 2, padding = 1)),nn.Sigmoid())(training_data[128].type(torch.FloatTensor))\n",
    "disc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrAeIWpNF2Wu"
   },
   "source": [
    "## Self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-mqql9SmF6gj"
   },
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "  def __init__(self,feature_in,feature_out):\n",
    "    super(SelfAttentionLayer,self).__init__()\n",
    "    self.Q = nn.Conv1d(feature_in, feature_out, kernel_size = 1, stride = 1, padding = 'same')\n",
    "    self.K = nn.Conv1d(feature_in, feature_out, kernel_size = 1, stride = 1, padding = 'same')\n",
    "    self.V = nn.Conv1d(feature_in, feature_out, kernel_size = 1, stride = 1, padding = 'same')\n",
    "    self.softmax  = nn.Softmax(dim=1)\n",
    "  def forward(self,x):\n",
    "    Q = self.Q(x)\n",
    "    K = self.K(x)\n",
    "    V = self.V(x)\n",
    "    d = K.shape[0]\n",
    "    QK_d = (Q@K.permute(0,2,1))/(d)**0.5\n",
    "    prob = self.softmax(QK_d)\n",
    "    attention = torch.Tensor(prob @ V)\n",
    "    return attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWLcHGfszGaG"
   },
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5gs-K07lxc1b"
   },
   "outputs": [],
   "source": [
    "from Bio.SeqIO.QualityIO import log\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Any\n",
    "\n",
    "HYPERPARAMETERS: Dict[str, Any] = {\n",
    "    \"r1_w\": 0.2,\n",
    "    \"r2_w\": 0.2,\n",
    "    \"dra_w\": 0.1,\n",
    "    \"rlc_af\": 1.,\n",
    "    \"rlc_ar\": 1.,\n",
    "    \"rlc_w\": 0.15\n",
    "}\n",
    "\n",
    "class Regularization (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regularization, self).__init__()\n",
    "\n",
    "    def RLC(self, discriminator_prediction_real: torch.Tensor, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        regularization_loss = (discriminator_prediction_real - HYPERPARAMETERS[\"rlc_af\"]).norm(dim=-1).pow(2).mean() \\\n",
    "                              + (discriminator_prediction_fake - HYPERPARAMETERS[\"rlc_ar\"]).norm(dim=-1).pow(2).mean()\n",
    "        return HYPERPARAMETERS[\"rlc_w\"] * regularization_loss\n",
    "\n",
    "    def R1(self,discriminator_prediction_real):\n",
    "        #grad_real = torch.autograd.grad(outputs=discriminator_prediction_real.sum(), inputs=real, create_graph=True)[0]\n",
    "        regularization_loss: torch.Tensor = HYPERPARAMETERS[\"r1_w\"] * discriminator_prediction_real.pow(2).view(discriminator_prediction_real.shape[0], -1).sum(1).mean()\n",
    "        return regularization_loss\n",
    "\n",
    "    def R2(self, discriminator_prediction_fake):\n",
    "        #grad_fake = torch.autograd.grad(outputs=discriminator_prediction_fake.sum(), inputs=fake, create_graph=True)[0]\n",
    "        regularization_loss: torch.Tensor = HYPERPARAMETERS[\"r2_w\"] * discriminator_prediction_fake.pow(2).view(discriminator_prediction_fake.shape[0], -1).sum(1).mean()\n",
    "        return regularization_loss\n",
    "    \n",
    "\n",
    "class GANLossDiscriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(GANLossDiscriminator, self).__init__()\n",
    "\n",
    "    def Least_square(self, discriminator_prediction_real: torch.Tensor,discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        return 0.5 * ((- discriminator_prediction_real - 1.).pow(2).mean()+ discriminator_prediction_fake.pow(2).mean())  \n",
    "\n",
    "    def non_saturation(self, discriminator_prediction_real: torch.Tensor,discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softplus(- discriminator_prediction_real).mean() + F.softplus(discriminator_prediction_fake).mean()   \n",
    "\n",
    "\n",
    "    def Basic(self, discriminator_prediction_real: torch.Tensor,discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:         \n",
    "        return - torch.mean(torch.log(discriminator_prediction_real + 1e-8) + torch.log(1 - discriminator_prediction_fake + 1e-8))\n",
    "\n",
    "    def Wasserstein(self, discriminator_prediction_real: torch.Tensor,discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        return  -(torch.mean(discriminator_prediction_real) - torch.mean(discriminator_prediction_fake))\n",
    "\n",
    "\n",
    "class GANLossGenerator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(GANLossGenerator, self).__init__()\n",
    "\n",
    "    def Least_square(self, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        return 0.5 * (discriminator_prediction_fake - 1.).pow(2).mean()\n",
    "\n",
    "    def Regular(self, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:   \n",
    "        return  F.softplus(discriminator_prediction_fake).mean()\n",
    "\n",
    "    def non_saturation(self, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor: \n",
    "        return F.softplus(- discriminator_prediction_fake).mean()\n",
    "\n",
    "    def Basic(self, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor: \n",
    "        return - torch.mean( torch.log(1 - discriminator_prediction_fake + 1e-8))\n",
    "\n",
    "    def Wasserstein(self, discriminator_prediction_fake: torch.Tensor) -> torch.Tensor:\n",
    "        return -torch.mean(discriminator_prediction_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QdWA_mPztfk"
   },
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CUmTaVi1z2BC"
   },
   "outputs": [],
   "source": [
    "def Initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv1d,nn.ConvTranspose1d,nn.BatchNorm1d)):\n",
    "            nn.init.normal_(m.weight.data,0,0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-L4zYqoshS0"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0MQWNmOaTuo",
    "outputId": "ca3023dd-e793-4948-aee2-ff86a7a7152c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 34/133 [13:03<39:08, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Batch 34/133                 Loss D: 2.33, loss G: 1.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 41/133 [16:12<36:22, 23.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\DEBI\\Uottawa\\Final project - Protienea\\Synthetic promoters via GAN\\Synthetic promoters via GAN.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m disc\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Backpropagate and optimize the discriminator\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m loss_disc\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m opt_disc\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#X51sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Calculating the loss of the generator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EGYPT_LAPTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\EGYPT_LAPTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Learning_rate = 1e-5\n",
    "Batch_size = 128\n",
    "seq_size = 4\n",
    "seq_len = 600\n",
    "noise_dim = 200\n",
    "num_epochs = 50\n",
    "features_disc = 1200\n",
    "features_gen = 200\n",
    "\n",
    "# lists\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "# Using dataloader to splitting the data into batches\n",
    "dataloader = DataLoader(training_data, batch_size=Batch_size, shuffle=True)\n",
    "# Calling Generator and discriminator classes\n",
    "gen = Generator(noise_dim, features_gen,SelfAttentionLayer)\n",
    "disc = Discriminator(seq_len, features_disc,SelfAttentionLayer)\n",
    "# Initialize the weights of two classes: mean = 0, std = 0.02\n",
    "Initialize_weights(gen)\n",
    "Initialize_weights(disc)\n",
    "\n",
    "# Using Adam optimizers\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=Learning_rate, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=Learning_rate, betas=(0.5, 0.999))\n",
    "# calling Generator and Discrimiator losses\n",
    "lossG = GANLossGenerator()\n",
    "lossD = GANLossDiscriminator()\n",
    "reg = Regularization()\n",
    "\n",
    "# Training models\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real in enumerate(tqdm(dataloader)):\n",
    "        real = real.to(device)\n",
    "        # Generate noises\n",
    "        noise = torch.randn(len(real),noise_dim,1).to(device)\n",
    "        # path the noise into generator\n",
    "        fake = gen(noise)\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        # calcuate the loss of discriminator\n",
    "        loss_disc = lossD.Basic(disc_real,disc_fake)\n",
    "        loss_disc = torch.autograd.Variable (loss_disc,requires_grad=True)\n",
    "        loss_disc_RLC = torch.Tensor(reg.RLC(disc_real,disc_fake))\n",
    "        loss_disc = loss_disc + loss_disc_RLC\n",
    "        # using zero grad to avoid accumelation of the gradient\n",
    "        disc.zero_grad()\n",
    "        # Backpropagate and optimize the discriminator\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # Calculating the loss of the generator\n",
    "        loss_gen = lossG.Basic(disc_fake)\n",
    "        loss_gen = torch.autograd.Variable (loss_gen,requires_grad=True)\n",
    "        # backpropagate and optimize the generator\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally\n",
    "        disc_losses.append(loss_disc.detach().float())\n",
    "        gen_losses.append(loss_gen.detach().float())\n",
    "        if batch_idx == 33:\n",
    "          print(\n",
    "              f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx+1}/{len(dataloader)} \\\n",
    "                Loss D: {loss_disc:.2f}, loss G: {loss_gen:.2f}\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(),\"DCGAN Generator.pt\")\n",
    "torch.save(disc.state_dict(),\"DCGAN Discriminator.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize(gen_losses,disc_losses):\n",
    "    Gloss = []\n",
    "    Dloss = []\n",
    "    for i in range(len(gen_losses)):\n",
    "        Gloss.append(gen_losses[i].detach().numpy())\n",
    "        Dloss.append(disc_losses[i].detach().numpy())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(Gloss,label=\"Generator \")\n",
    "    plt.plot(Dloss,label=\"Discriminator\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAFNCAYAAABMhmimAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8kklEQVR4nO3deXwV5dn/8c9FSNhlV/ZFAdkJGBBEheJWrVstrlTRqpS26uNWtfVpi/axP9daq1TrvlZtsS612mpV1Cooi1F2QURBEMIOQvbr98cM8QSynCTnZHKS7/v1mlfOuWe75j5zkiv3fc+MuTsiIiIiEo1GUQcgIiIi0pApGRMRERGJkJIxERERkQgpGRMRERGJkJIxERERkQgpGRMRERGJkJIxkQbKzKaZ2ZM13MZOMzswUTGF23zVzCZXc937zOxXiYxHymZmPcLPPy3qWMpjZr80swcTvaxIopnuMyapxMzOAq4ABgPfAJ8DjwH3eh07mc1sJvCku9fJX/BmNg3o4+4/LGPeeOBNYFdYtBV4H7jN3efUToTRMbNeBOdWursXJmib4wnOh26J2F4V9+0En6UDeUA2cL+7P1vbsVTGzF4FjgjfNiGIOT98/6S7T40kMJEkUsuYpAwzuwq4C7gN6AQcAEwFxgIZtRxL4yRv38ws6u/nWndvCbQCRgNLgXfN7Khk7KyOHHNCJPv8qKZh4ed5MPAocI+Z/aY6G0rm8bn78e7eMoz1KeDWPe9jE7E6Wsci1VIvfvFJ/WdmrYEbgZ+6+wx33+GBj9x9krvnhcs1MbPbzexLM1sfdls1C+eNN7M1ZnaVmW0ws3VmdkHMPuJZ91oz+xp4xMzamtnLZpZjZlvC193C5W8i+O/+nrAr556w/DAzm2Nm28Kfh8Xsf6aZ3WRm7xG0YuzT/Wdm15nZZ2a2w8wWm9n3Y+adb2b/DY9hi5l9bmbHx8zvbWZvh+u+DnSIp+7Del7j7r8GHgRuidmmm1mf8PUJYUw7zOwrM7s6ZrlTzCzbzLaH8X+3vGMOyy6KOab3zOxOM9tqZivDOjzfzFaHn+PkmP08amb/F+fn/T0z+yiMaXXYUrjHO+HPreHnN8bMGpnZ/5rZF+H2Hg/PS8ysV1gXF5rZlwStinEzswHhcW81s0VmdnLMvDLr1cw6hOfcVjPbbGbvWhzJrLtvdPcngJ8AvzCz9uH2VpnZ0TH7LenGLuv4Ysoah8vMNLPfhp/XDjN7zcw6xGzvvLDuNpnZr/beX5z15Gb2MzNbDiwPy+4KP7/tZjbPzI6IWb6sY5hswXd8o5ldX81lm5nZYxZ8z5aY2TVmtqYqxyISS8mYpIoxBF0WL1ay3M1APyAT6AN0BX4dM78T0DosvxCYbmZtq7BuO6AnMIXg+/NI+L4HsBu4B8DdrwfeBS4J/6O/xMzaAf8E/gi0B34P/HPPH8PQueG2WwFflHF8nxEkea2BG4AnzaxzzPxDgWUEidatwENmZuG8vwDzwnm/BaozLuvvwAgza1HGvIeAH7t7K4Ju5DcBzGwU8Djwc6ANcCSwKma9yo75UOATgjr7C/AMMJLgM/ohQcLbspx4K/q8vwHOC2P6HvATMzs1nHdk+LNN+PnNAs4Pp+8QJMotCT/vGOOAAcBx5cSzDzNLB/4BvAbsD1wKPGVmB4eLlFmvwFXAGqAjQSvxLwm69OL1ItAYGFWFdSo7vnOACwiOIwPYkzgOBP4ETAI68+1nUh2nEpwTA8P3cwi+s+0Izo+/mVnTCtY/nKB18Cjg12Y2oBrL/gboRXAeHENwHopUm5IxSRUdgI2x43fM7P2wVWC3mR0ZJh1TgCvcfbO77wB+B5wVs50C4EZ3L3D3V4CdwMFxrlsM/Mbd89x9t7tvcvfn3H1XuPxNBH+syvM9YLm7P+Huhe7+NEHX30kxyzzq7ovC+QV7b8Dd/+bua929OBzvs5zSf0y/cPcH3L2IYCxdZ+AAM+tBkMD8Koz/HYIEoKrWAkaQwOytABhoZvu5+xZ3nx+WXwg87O6vh3F/5e5L4z1m4HN3fyQ8pmeB7gSfYZ67v0YwnqhPOfGW+XkDuPtMd18QxvQJ8DQVf36TgN+7+0p33wn8AjjLSneXTXP3b9x9dwXb2dtogsTuZnfPd/c3gZeBs2OOoax6LSD4fHuGx/duVcZNhnW9kSCJiVdlx/eIu38azv8rQZIEMBH4h7v/193zCf7Jqe4Yz/8Xfkd3A7j7k+F3sdDd7yD4p+3gCta/Ifz+fgx8DAyrxrJnAL8LP481BP9giVSbkjFJFZuADrF/+Nz9MHdvE85rRNBC0ByYFyZpW4F/heUl29lrQPYugj+E8ayb4+65e96YWXMz+3PY9bKdoGurjZV/dVkX9m35+YLSLQSrK6iDPV092TExDqZ0d+PXe164+57B9y3DfW9x92/22ndVdSX4I7q1jHk/AE4AvrCgO3RMWN6doEWvPBUeM7A+5vWeP8B7l5XXMlbe542ZHWpmb1nQzbyNYPxhRV23e39+XxC0LB0QU1bZsZS33dXuXrzXtvecF+XV623ACuA1C7pvr6vKTsMWuY7A5iqsVtnxfR3zuqSuCY9xz4zw3NxUhf2WG4OZXR12FW4LvxOtqfhzLC/Gqixb6nj2jkmkqpSMSaqYRXAV2CkVLLOR4A/zIHdvE06tPRgIXJl41t37P/mrCP4DP9Td9+Pbri0rZ/m1BF2asXoAX1WwjxJm1hN4ALgEaB8mogtj9leRdUDbvboXe8Sx3t6+D8zfK6kDwN3nuPspBF1ULxC0jEDwh+qgCrYZ1VWwfwFeArq7e2vgPsr/7GDfz68HUEjpZLE6x7IW6L7XeK+S86K8evVg3ORV7n4gcDJwpVXt4opTwvg/DN9/Q/APyR6dylinup/VOqDkKlILxmK2L3/xCpXEEI4Pu4agpapt+J3YRnzfiZoodTwE/3CIVJuSMUkJ7r6VYIzUn8xsopm1smBAdSbQIlymmCBZudPM9gcws65mVun4nWqu24oggdsajgfb+8q09ZQehP8K0M/MzjGzxmZ2JsG4l5criy/UguAPUU4Y3wUELWOVcvcvgLnADWaWYWaHU7p7tFwW6GrBlXcXEYxN2nuZDDObZGatw+6v7QTduhCMebrAzI4KP7OuZtY/nn0nWStgs7vnhuPazomZl0MQf+zn9zRwhQUXQrQk6MZ+1qt46wszaxo7ESRDu4BrzCzdgltgnAQ8U1G9mtmJZtYn7GLfBhTxbZ1XtP92ZjYJmA7c4u57WqiyCbpd080si6BrMVFmACdZcPFFBjCNxCRMrQgSyhygsZn9GtgvAdutzF8JLn5oa2ZdCf5BEqk2JWOSMtz9VuBKgv+E14fTn4FrCe6BRfh6BTA77Dr8DxWPH4lV1XX/ADQjaFWbTdCtGesuYKIFV1z9MfyjdyJBi9qm8DhOdPeN8QTn7ouBOwhaCdcDQ4D34js0IEg2DiXolvoNwaD6inQxs50E46zmhPsbH47TKsu5wKqw7qYSjLHC3T8kGNR9J0HS8Db7thBG4afAjWa2g2AM056WvD3daDcB74VdwqOBh4EnCLqjPwdyCQbbV0VXggQ+dupOkHwdT3Au/Qk4L2ZcXZn1CvQlOEd3EpwTf3L3tyrY98fh57mCIKm+woMrZPf4FUEL5haCf3z+UsVjK5e7LyKoq2cIWpV2AhsIWrtr4t8E37tPCbp2c6mdLsMbCS6e+JzgM5hBzY9FGjDd9FVERGpV2LK4Fejr7p9HHE6NmdlPgLPcvaILQETKpZYxERFJOjM7KbzopQVwO7CA0rc4SRlm1tnMxobd7gcTtHY/H3VckrqUjImISG04heBihbUEXaxnVeVWHHVMBsEQiR0E9317kaB7WaRa1E0pIiIiEiG1jImIiIhESMmYiIiISIRS4qn3HTp08F69ekUdhoiIiEil5s2bt9HdO1a+ZCAlkrFevXoxd+7cqMMQERERqZSZVelxc+qmFBEREYmQkjERERGRCCkZExEREYlQSowZExERacgKCgpYs2YNubm5UYciMZo2bUq3bt1IT0+v0XaUjImIiNRxa9asoVWrVvTq1QszizocAdydTZs2sWbNGnr37l2jbambUkREpI7Lzc2lffv2SsTqEDOjffv2CWmtVDImIiKSApSI1T2J+kyUjImIiEil1q9fzznnnMOBBx7IIYccwpgxY3j++ecji2fmzJm8//77ke0/kZSMiYiISIXcnVNPPZUjjzySlStXMm/ePJ555hnWrFmT1P0WFhaWO686yVhF24uSkjGAjSvggz9HHYWIiEid9Oabb5KRkcHUqVNLynr27Mmll14KQFFRET//+c8ZOXIkQ4cO5c9/Dv6mzpw5k/HjxzNx4kT69+/PpEmTcHcA5s2bx7hx4zjkkEM47rjjWLduHQDjx4/n8ssvJysri7vuuot//OMfHHrooQwfPpyjjz6a9evXs2rVKu677z7uvPNOMjMzeffdd1m1ahUTJkxg6NChHHXUUXz55ZcAnH/++UydOpVDDz2Ua665pjarLW66mhJgxX/gX9dCn6Oh/UFRRyMiIlKnLFq0iBEjRpQ7/6GHHqJ169bMmTOHvLw8xo4dy7HHHgvARx99xKJFi+jSpQtjx47lvffe49BDD+XSSy/lxRdfpGPHjjz77LNcf/31PPzwwwDk5+eXPAZxy5YtzJ49GzPjwQcf5NZbb+WOO+5g6tSptGzZkquvvhqAk046icmTJzN58mQefvhhLrvsMl544QUguBr1/fffJy0tLYm1VH1KxgAO/m6QjL1/N5z0h6ijERERKdcN/1jE4rXbE7rNgV324zcnDYp7+Z/97Gf897//JSMjgzlz5vDaa6/xySefMGPGDAC2bdvG8uXLycjIYNSoUXTr1g2AzMxMVq1aRZs2bVi4cCHHHHMMELSsde7cuWT7Z555ZsnrNWvWcOaZZ7Ju3Try8/PLvY3ErFmz+Pvf/w7AueeeW6oV7PTTT6+ziRgoGQu07RX8nPeIkjEREZG9DBo0iOeee67k/fTp09m4cSNZWVlAMKbs7rvv5rjjjiu13syZM2nSpEnJ+7S0NAoLC3F3Bg0axKxZs8rcX4sWLUpeX3rppVx55ZWcfPLJzJw5k2nTplU5/tjt1UVKxvaWuw2ato46ChERkTJVpQUrUSZMmMAvf/lL7r33Xn7yk58AsGvXrpL5xx13HPfeey8TJkwgPT2dTz/9lK5du5a7vYMPPpicnBxmzZrFmDFjKCgo4NNPP2XQoH2Pbdu2bSXbeuyxx0rKW7Vqxfbt37YQHnbYYTzzzDOce+65PPXUUxxxxBE1Pu7aogH8e5s1PeoIRERE6hQz44UXXuDtt9+md+/ejBo1ismTJ3PLLbcAcNFFFzFw4EBGjBjB4MGD+fGPf1zhlYsZGRnMmDGDa6+9lmHDhpGZmVnulZHTpk3j9NNP55BDDqFDhw4l5SeddBLPP/98yQD+u+++m0ceeYShQ4fyxBNPcNdddyW2EpLI9lzVUJdlZWX5noF8STMtpjVs2rbk7ktERKQKlixZwoABA6IOQ8pQ1mdjZvPcPSvebahlTERERCRCSsbKsnpO1BGIiIhIA6FkrCzv3hF1BCIiItJAKBkry6evRh2BiIiINBBKxspTVBB1BCIiItIAKBkrz/zHo45AREREGgAlY+VZ9krUEYiIiNQZaWlpZGZmMmjQIIYNG8Ydd9xBcXExAHPnzuWyyy6r8T7uu+8+Hn+8ao0hhx12WLX39+ijj7J27dpqr58ougN/eVb8J+oIRERE6oxmzZqRnZ0NwIYNGzjnnHPYvn07N9xwA1lZWSWPRqquwsJCpk6dWuX1yrtZbDweffRRBg8eTJcuXeJep6ioKOHPuVTLmIiIiFTJ/vvvz/33388999yDuzNz5kxOPPFEAN5++20yMzPJzMxk+PDh7NixA4BbbrmFIUOGMGzYMK677joAxo8fz+WXX05WVhZ33XUX06ZN4/bbby+Zd8UVV5CVlcWAAQOYM2cOp512Gn379uV///d/S2Jp2bIlEDwHc/z48UycOJH+/fszadIk9tzY/sYbb2TkyJEMHjyYKVOm4O7MmDGDuXPnMmnSJDIzM9m9ezdvvPEGw4cPZ8iQIfzoRz8iLy8PgF69enHttdcyYsQI/va3vyW8PpOWjJlZUzP70Mw+NrNFZnZDWP6omX1uZtnhlJmsGGpse/RNlyIiInXRgQceSFFRERs2bChVfvvttzN9+nSys7N59913adasGa+++iovvvgiH3zwAR9//DHXXHNNyfL5+fnMnTuXq666ap99ZGRkMHfuXKZOncopp5zC9OnTWbhwIY8++iibNm3aZ/mPPvqIP/zhDyxevJiVK1fy3nvvAXDJJZcwZ84cFi5cyO7du3n55ZeZOHEiWVlZPPXUU2RnZ2NmnH/++Tz77LMsWLCAwsJC7r333pJtt2/fnvnz53PWWWclqgpLJLObMg+Y4O47zSwd+K+Z7blnxM/dfUYS950Y79wGJ94ZdRQiIiLfevU6+HpBYrfZaQgcf3NCNjV27FiuvPJKJk2axGmnnUa3bt34z3/+wwUXXEDz5s0BaNeuXcnyZ555ZrnbOvnkkwEYMmQIgwYNonPnzkCQCK5evZr27duXWn7UqFF069YNgMzMTFatWsXhhx/OW2+9xa233squXbvYvHkzgwYN4qSTTiq17rJly+jduzf9+vUDYPLkyUyfPp3LL7+80jhrKmktYx7YGb5ND6e6/yDMWHMfjjoCERGROmnlypWkpaWx//77lyq/7rrrePDBB9m9ezdjx45l6dKlFW6nRYsW5c5r0qQJAI0aNSp5ved9WQ8ij10mLS2NwsJCcnNz+elPf8qMGTNYsGABF198Mbm5uXEdY7xx1lRSB/CbWRowD+gDTHf3D8zsJ8BNZvZr4A3gOnfPK2PdKcAUgB49eiQzTBERkdSRoBasmsjJyWHq1KlccsklmFmpeZ999hlDhgxhyJAhzJkzh6VLl3LMMcdw4403MmnSJJo3b87mzZtLtY4l057Eq0OHDuzcuZMZM2YwceJEAFq1alUypu3ggw9m1apVrFixgj59+vDEE08wbty4WokxqcmYuxcBmWbWBnjezAYDvwC+BjKA+4FrgRvLWPf+cD5ZWVnRtagVF0GjxF41ISIikmp2795NZmYmBQUFNG7cmHPPPZcrr7xyn+X+8Ic/8NZbb9GoUSMGDRrE8ccfT5MmTcjOziYrK4uMjAxOOOEEfve739VK3G3atOHiiy9m8ODBdOrUiZEjR5bMO//885k6dSrNmjVj1qxZPPLII5x++ukUFhYycuTIal3dWR2250qDpO8oaAnb5e63x5SNB6529xMrWjcrK8vnzp2b3ACntS67/Ef/hh6jk7tvERGRCixZsoQBAwZEHYaUoazPxszmuXvc9/pI5tWUHcMWMcysGXAMsNTMOodlBpwKLExWDAnx4QNRRyAiIiL1WDK7KTsDj4XjxhoBf3X3l83sTTPrCBiQDdROG2B15VQ88FBERESkJpKWjLn7J8DwMsonJGufSbG+bjfciYiISGrTHfhFRERSQG2N8Zb4JeozUTIWj8L8qCMQEZEGrGnTpmzatEkJWR3i7mzatImmTZvWeFt6UHg8Vs+G3kdGHYWIiDRQ3bp1Y82aNeTk5EQdisRo2rRpyR3/a0LJWDxm36dkTEREIpOenk7v3r2jDkOSRN2U8chZEnUEIiIiUk8pGYvH5pVRRyAiIiL1lJIxERERkQgpGYuXrmARERGRJFAyFq+dG6KOQEREROohJWPxWvxC1BGIiIhIPaRkLF5L/hF1BCIiIlIPKRmL15Yvoo5ARERE6iElY/Ha9mXUEYiIiEg9pGRMREREJEJKxkREREQipGSsKgp2Rx2BiIiI1DNKxqpi25qoIxAREZF6RslYVfz3zqgjEBERkXpGyVhVrPsk6ghERESknlEyVhW6vYWIiIgkmJKxqsjdFnUEIiIiUs8oGRMRERGJkJIxERERkQgpGRMRERGJUNKSMTNramYfmtnHZrbIzG4Iy3ub2QdmtsLMnjWzjGTFkBS68auIiIgkUDJbxvKACe4+DMgEvmtmo4FbgDvdvQ+wBbgwiTEk3jc5UUcgIiIi9UjSkjEP7AzfpoeTAxOAGWH5Y8CpyYohKbavizoCERERqUeSOmbMzNLMLBvYALwOfAZsdffCcJE1QNdkxpBwb90UdQQiIiJSjyQ1GXP3InfPBLoBo4D+8a5rZlPMbK6Zzc3JqUNdgzs3RB2BiIiI1CO1cjWlu28F3gLGAG3MrHE4qxvwVTnr3O/uWe6e1bFjx9oIMz4aMyYiIiIJlMyrKTuaWZvwdTPgGGAJQVI2MVxsMvBismJIil0bo45ARERE6pHGlS9SbZ2Bx8wsjSDp+6u7v2xmi4FnzOz/gI+Ah5IYg4iIiEidlrRkzN0/AYaXUb6SYPyYiIiISIOnO/CLiIiIREjJmIiIiEiElIxVR1Fh5cuIiIiIxEHJWHXs3hx1BCIiIlJPKBmrjl1KxkRERCQxlIxVR/ZTUUcgIiIi9YSSser49N9RRyAiIiL1hJKx6vhGz6cUERGRxFAyVh27t0QdgYiIiNQTSsZEREREIqRkTERERCRCSsZEREREIqRkTERERCRCSsZEREREIqRkrLrco45ARERE6gElY9VVsCvqCERERKQeUDJWXbu3Rh2BiIiI1ANKxqord2vUEYiIiEg9oGSsuoryo45ARERE6gElY9U1609RRyAiIiL1gJKx6vpqXtQRiIiISD2gZKy6crdFHYGIiIjUA0rGqitve9QRiIiISD2gZKy6NIBfREREEiBpyZiZdTezt8xssZktMrP/CcunmdlXZpYdTickKwYRERGRuq5xErddCFzl7vPNrBUwz8xeD+fd6e63J3HfIiIiIikhacmYu68D1oWvd5jZEqBrsvYnIiIikopqZcyYmfUChgMfhEWXmNknZvawmbWtjRhERERE6qKkJ2Nm1hJ4Drjc3bcD9wIHAZkELWd3lLPeFDOba2Zzc3Jykh2miIiISCSSmoyZWTpBIvaUu/8dwN3Xu3uRuxcDDwCjylrX3e939yx3z+rYsWMywxQRERGJTDKvpjTgIWCJu/8+prxzzGLfBxYmK4akKyqMOgIRERFJccm8mnIscC6wwMyyw7JfAmebWSbgwCrgx0mMIbnytkPzdlFHISIiIiksmVdT/hewMma9kqx91jolYyIiIlJDugN/TeTqkUgiIiJSM0rGaqKoIOoIREREJMUpGauJ+Y9FHYGIiIikOCVjNfHZm1FHICIiIilOyVhN5GnMmIiIiNSMkrGayNsZdQQiIiKS4pSM1YQXRR2BiIiIpDglYyIiIiIRUjImIiIiEiElYyIiIiIRUjImIiIiEiElYyIiIiIRUjImIiIiEiElYyIiIiIRUjImIiIiEiElYzVVrBu/ioiISPUpGaspPZ9SREREakDJWE3lKhkTERGR6lMyVlN5O6KOQERERFKYkrGayv8m6ghEREQkhSkZq6lZd0cdgYiIiKQwJWM1teq9qCMQERGRFKZkrKbUTSkiIiI1oGSsporyoo5AREREUpiSMREREZEIxZWMmVkLM2sUvu5nZiebWXol63Q3s7fMbLGZLTKz/wnL25nZ62a2PPzZtuaHISIiIpKa4m0ZewdoamZdgdeAc4FHK1mnELjK3QcCo4GfmdlA4DrgDXfvC7wRvhcRERFpkOJNxszddwGnAX9y99OBQRWt4O7r3H1++HoHsAToCpwCPBYu9hhwajXiFhEREakX4k7GzGwMMAn4Z1iWFu9OzKwXMBz4ADjA3deFs74GDoh3OyIiIiL1TbzJ2OXAL4Dn3X2RmR0IvBXPimbWEngOuNzdSz3I0d0d8HLWm2Jmc81sbk5OTpxhioiIiKSWxvEs5O5vA28DhAP5N7r7ZZWtFw7yfw54yt3/HhavN7PO7r7OzDoDG8rZ5/3A/QBZWVllJmwiIiIiqS7eqyn/Ymb7mVkLYCGw2Mx+Xsk6BjwELHH338fMegmYHL6eDLxY9bBFRERE6od4uykHhl2MpwKvAr0JrqisyNhwmQlmlh1OJwA3A8eY2XLg6PC9iIiISIMUVzclkB52OZ4K3OPuBWZWYdehu/8XsHJmHxV/iCIiIiL1V7wtY38GVgEtgHfMrCewvcI1RERERKRScSVj7v5Hd+/q7id44AvgO0mOLXUUFUYdgYiIiKSoeAfwtzaz3++51YSZ3UHQSiYA+TujjkBERERSVLzdlA8DO4Azwmk78Eiygko5+d9EHYGIiIikqHgH8B/k7j+IeX+DmWUnIZ7UpGRMREQk6eb+8wG6zL2tWuu2Kd7KhrQDKPr+Axw0ZHSCI6uZeJOx3WZ2eHiFJGY2FtidvLBSTP6OqCMQERGp97LmXF39lQ16FX/JvNVLIEWTsanA42bWOny/hW9v3CoFuVFHICIiIikq3qspP3b3YcBQYKi7DwcmJDWyVDL7T1FHICIiIikq3gH8ALj79piHfV+ZhHhS04o3oo5AREREUlSVkrG9lHd3/YanUMPnREREpHpqkoxV+DgkEREREalchQP4zWwHZSddBjRLSkQiIiIiDUiFyZi7t6qtQEREREQaopp0U4qIiIikFKuDI96VjImIiIhESMmYiIiISISUjImIiIhESMmYiIiISISUjImIiIhESMmYiIiISISUjImIiIhEqMKbvoqIiEhybducw5Jnrid9dw4e89hnw8t9bzEPx3Gs5P3e870aj5GuaL+1GUdZcR1S461QJ280pmQsUQrzoXFG1FGIiEiK+ereUxldsDDqMCRC6qZMlPydUUcgIiIpqEvBqqhDkIglLRkzs4fNbIOZLYwpm2ZmX5lZdjidkKz917q8HVFHICIiKSgRXXiS2pLZMvYo8N0yyu9098xweiWJ+69dahkTEZFqUTLW0CVtzJi7v2NmvZK1/Ton/5uoIxARkWqY/fRNjF52a2T7bxvZnqWuiGLM2CVm9knYjVl/zsG87VFHICIi1dD10yejDkEauNq+mvJe4LeAhz/vAH5U1oJmNgWYAtCjR4/aiq/6/vVLuOToqKMQEWkwvvw0m40v/RrzoqDAvVq3LRjuaxMcmdRl1tBvbeHu6/e8NrMHgJcrWPZ+4H6ArKwsL2+5OmPjsqgjEBFpUHr8ZRwp8K+6SKVqNRkzs87uvi58+31AN1YREWnAtuSsY/N9J9CzcFWV121c9xo4RKolacmYmT0NjAc6mNka4DfAeDPLJOimXAX8OFn7FxGR6HhxMXm5uypdbt0DpzOwaKUuKJQGLZlXU55dRvFDydqfiIjUHfPv/AGH7Hiz0uUG1kIsInWdHockIiJVMufOMxm57V8VLpOQZwiKNBBKxkREZB9LbhrDgILFZc4bWcuxiNR3SsZERBqInLWryPlyaamygf86s8xlB9RGQCJRsLSoI9iHkjERkXpm3RfL2PHED2lduKlU+QFsomNEMYnUFX1Hfy/qEPahZExEJMVsyVnHpzOfguJCALwwn9HL7yiZ3zmcRBqizxv15Ouux5Q5r+3g4+i/X917+I+SMRGROuiTmc+Ru2Utxbnb93luYlvg0GjCEqkTPmh/Code+niZ83qHUypRMiYiEqHCgnzmPPVr0rZ9yagt/ywpHxphTCJRmD/mHjJatsFohFNMkxZtOGjIYTRK23eMV337Z0TJWCJV87loIlK/FeTnsXT2K+xav5Kshb8lzb59wltjYEx0oYkk3ZzWx9J40Ckl79t07UvvQfumUyNqM6g6RslYIhXshozmUUchIhHYtXMbS9/9O40/fpKhuXNLzUsHhux5o//XpB75sO2J9Dn72270Js1a0KJVm1LL6FYolVMylkgFu5SMidRjebm7WP/lp+zYuJbdsx8ia/t/SuY1p2H/Zy/1y6zel5DWsgPprToydMJZpDUuO10YVctx1VdKxhIpfye06BB1FCKSAJ8v+oBNqz4hfeHfGLb7AwCaAD2iDUukxmZ1/iFNemTRtG1n+o88pswxWeo6r11KxhIpv/KH4opI3bFj22bWr1pMztznOXj1X2nH9pJ5qXhFlsi8luMp6jWOgcdeQOPG6TRt3nKfZZRo1T1KxhIp/5uoIxCRMuTu2snnC96j6M3fMTgvu6S8VTj1iSowkSr6oONEvN1BZJ5yGY3TM2icnlFqvp4JmpqUjCVSgZIxkSgVFRayfs1nrP7gebovfYguvgGApujxPlL3LcoYwo7W/Wk35od07H4wbTvue+ve+nZLBwkoGUukwryoIxBpEL5auYQ1c16k3fIZ9C1cXlKeBnQJJ5G64MtGXdnRuB0G7Gh1EE36H8fg8RNJS2uMNWpUatlB0YQodYCSsUR69Vrod1zUUYjUG3m5u1j8zt/JX/Y6h256oaS8aziJ1BWzDzib9qPP4aChY0sNiNcFHxIPJWOJtOXzqCMQSUmFBfl8vnA2m95/nNE5fyspbwIMjy4sEQCWpg9ka5tBNO03gU4Hj6RD5577jNUaHVFsUj8oGRORWpOfl8vKBe+xZd4L9F33Eh3YCgS/iPqGk0ht+7DNCTQ/5Cz6jTqOjCZN95nfP4KYpGFRMiYiSbF149eseP95Wi54nP4FiwHIQH/YpHYVeBrzO55Ki+Hfp+fgsbRq3W6fZXTjUomakjERqZHioiI+W/A+m2Y/zdB1M2huwYUsbYCsSCOThuLTxv3YfOApHDhuEh069Sg1ZisdXYEodZ+SMRGJ25acdSz/799otvR5huTNB6ARMV2Meu6iJNji9MHsHv4jDhr1Pdp06FTmMv1qOSaRRFMyJiJl+mrlIla/8xRDP3+wpLWrLerSkcT6mo6s6nQs+2WeTK/BY2jesnWp+QMjikukNikZE2ngvLiYr1YuZvWb9zNm7WMl5SW3j1Brl9TQ/JZHkjbiXPodejzNWrQqNa9TOIk0ZErGRBqYTevX8Om//sSYz6cDQa7VLZxEquOTpiPZ1fM7dBg4jp4DRpKe0aTU/BERxSWSKpSMidRjaz9fyhev3c2YdU+WlLVHDwqWqlltXVjb/3x6H34G+3fd9/HpQyOISaQ+SVoyZmYPAycCG9x9cFjWDngW6AWsAs5w9y3JikGkocjd/Q2LZ/6V9OzHSwbWgx4NJPGbu98xpA8/k97DJ7Bfm/al5nUPJxFJjmS2jD0K3AM8HlN2HfCGu99sZteF769NYgy1r7gY9nremEgiFRcVsXLhLDa980DJI4Kaoq4gqdha258vup1Ct3GT6dp7YKnbP4BuQyISpaQlY+7+jpn12qv4FGB8+PoxYCb1LRnL3wlN94s6CqlHvtmxlcVvPEnP7DvYn800AvqEk0isWZ3Po9uEi+h64OB9ki21korUXbU9ZuwAd18Xvv4aOKC8Bc1sCjAFoEePFHrUat52JWNSbbm7v2HZ+/+g6KO/MGLn2wC0AEZGG5ZEbJc3YUGH79Jk4Al0PngkHTr1JK3xvr++NRZQJDVFNoDf3d3MvIL59wP3A2RlZZW7XJ2Tux1aV76YSO6unSz8z5N0yJ5Or+IvgaC7cVi0YUmEZu9/BgeMu4ie/bNKtWw1R3eRF6nPajsZW29mnd19nZl1BjbU8v6TL2971BFIHbV6+cd89fp0Rm94FggSL43TaVjyPY15vS6m+xHn0rlX/31at0ZHFJeIRKu2k7GXgMnAzeHPF2t5/8mXvzPqCKQO2LVzGwv//Qi9F/yBjgQXDOuKtPpvjXXiq/4XcNARZ9O+U3dsr4t5MlBXoojsK5m3tniaYLB+BzNbA/yGIAn7q5ldCHwBnJGs/UdmzsPQ5+ioo5Ba5MXFLM9+l+0z/0jW9v8AQbeSHhtUP61q1IOv+5xJjzET6dyzX6mESzfPFZHqSObVlGeXM+uoZO2zTlj2z6gjkCTbtmUjS197iAFL/sB+7MLQg4rrm8Xpg9k1bDJ9Rp+8z8Ope4WTiEii6A78IpVY+/lSvnz1DkZv+CsQXJ+hwdSpba0dwBe9TqfLqNPo2mcIjdMzSs3Xw6lFpDYpGROJkbtrJ8tmv4x9+ABDc+cCuj9TKpt9wNl0Gn8xPfpllro6UZ+piNQlSsakwfLiYpbNf4u8N25h2O4PAN1aIpVspSVLuvyAtiNOoUf/LJq33PeeMro6UURSgZIxaVC+WrmENa/cxqEbn8OA/lEHJBXK9zTmdz6Ljof9kF4DR5W6FUQbdGWiiNQPSsak3iosyGfRuy+w33//j97FXwDQNZyk7lhjnVjTZxI9Djtjn6sTM1DrlojUf0rGpF5Z/tE7FL5yHQMKFtEYdTnWFcvT+rB16IX0O/wHtG5f+ilouh2EiDR0SsYkZRUXFbHg7edo8d7N9Cn6DIC+EcfUkM1t9R3SR0yiT9YxtGjVptQ8fS4iIuVTMpYMxUXQKK3y5aRKCvLzWDjzb7T64Pf0KfqMRqjlqzZtoB2f9TiDzmNOp3vfzH0e5aNHO4mIVI+SsWTI3QbN20UdRcrz4mK+XPYR21+4miF580kHhkcdVD23nRYs6j6JLodPonufIaVuB7F/OImISGIpGUuGXZuVjFVDYUE+n855ndz3/8yInW9jQM+og6qnPmxzAq2PnEqfoWNLtXDth65QFBGpbUrGkmH3lqgjSBkb167is5du5tCvn6YxuvN5Is1veSTNjryMvsPH7XOHeT03U0Sk7lAylgw5S6H7yKijqJOKCgtZ8v7LNH37RvoUfUYHoEPUQaWoLbRiadeJdD7iPHr2yyx1SwiAERHFJSIiVaNkLBleugRGnBt1FHXGpvWrWf78/2P010+RBgyOOqAUs6DJCIpH/5T+h51IkybNSsrboi5FEZH6QMmYJFzurp1kz7iF0Sv/CED7cJKyfeNN+aTL6bQbeTq9Bx1KRpOmpeYPiSguERGpHUrGJCG2bVrPp09ewcgt/6Qpumt6WT5qfhiNRl1Ev5HH0qxFq5LyFqiFS0SkIVMyJtW2af0aVj92EZm7ZtEa0Cg5WNmoFxuHXETfI06nbYdOpebpthwiIlIWJWMSt6LCQrJfe5TeH95AO7Y32O7Hb7wpH/c4j57fuZAuvUo/S/HAcBIREYmXkrE9Tr0PXpiauO25g1nithcRLy5m6Yf/ZsC/ziINOCTqgGrRovTB5I6YwoAjv0/zFvuVlLcADosuLBERqWeUjO2ReTYMPQNm3gzv3Frz7eVth6ata76dCOTn5ZL9j3sYtfC3GDAg6oCSaG6LcbQc/z8clHkE6Xvdi2tQRDGJiEjDomQsVqM0mHA9HH45/K5Lzba1Y31KJWPFRUUsePNphr33MzKofzcFnd3uFFqPPo8Dh40tdXsIPU9RRESipmSsLBkt4Ddb4dVr4MP7q7eNnV9Dx34JDSvh3Fm54H2aPz+ZTp5TLx66/X6HiXQ++hJ69RtWaiyXru4UEZG6SslYeczghNugQz945eqqr7/ybeh9ZOLjSoCvv1hK00eOog07U3Kw+WrrzOr+FzLgqPNo2+GAUvM0lktERFKNkrHKjLoYdnwN795etfXevR2O+lVyYqqGosIC5v/tZkYuu51OlS9eJ+zwZnxy8KUMOn4qbdp+e91m93ASERGpD5SMxeOoX8GyV2HDoqgjqbIV89+k40vn0pqddfY+YEvSDmbHIT9j2FFn0iTm7vOtgLHRhSUiIlIrIknGzGwVsAMoAgrdve6Po/7xO/Db1LirVlFBPtkPTOWQDc/RJ+pg9vLOAedx0PeuoEv33lh464/6fLWmiIhIZaJsGfuOu2+McP9Vk9YYJj4CMy6IOpJy7d6xlc//dBoDd8+L/H5gb3c8h57HX07P3v1Kki6AujmKTkREJDrqpqyKwadVLRkr2A3pzSpfroY2f/0F7e4bSjNgYNL3tq83O1/MwFOuoFOnriVl4yKIQ0REJBVFlYw58JqZOfBnd6/m/SMiMGUm3D8+vmU3LofOQ5MShhcXkz3jZoYvvoV2SdnDvpZ7N3IOv5GhY4+nZfPmJeUTamn/IiIi9VFUydjh7v6Vme0PvG5mS939ndgFzGwKMAWgR48eUcRYti5VeNzz4heTkowteeNxBrx7aVIfPL3NmzProCs45IQL6dghGCvXN5xEREQkcczdow3AbBqw093LvXdEVlaWz507t/aCqsyCGfDchfEtO21bwna7ZvFsuv31uIRtL9a/m53AgWfcRN/eqXjnMRERkbrDzOZV5eLEWm8ZM7MWQCN33xG+Pha4sbbjqJEhE+NPxmrKnS/nv0aPf5xBtwRtco134NNx9zJu/DGkNQoG1ycnxRMREZHKRNFNeQDwfHiFXWPgL+7+rwjiqJnuo2H17MqXKyqAtPRq7WLDivns/+R3qGkn7UfFffh85K858bsnkpGeRjdIWGInIiIiNVPryZi7r4R68BjEs5+GW3tXvtzy16H/CVXatBcVsOruk+i9dVa1QltVfAAff+cRThp3GI0aGcMhqePLREREpPp0a4vqah7nNYzPnF2lcWNrPnyRbq+cRxxpXinTmMKkH19P385t6AX0quL6IiIiEg0lYzVx5DXwzq2VL+cePHi8AoW7d9D4lm5xdx8+Uzie9JN+z/dHHkijRsa0ONcTERGRuqVR1AGktCOuim+5Dyu+jdqmuX+n8S2Vp2FPFB7NlX1eIff6zZz1fy/yg0MPolGjipM8ERERqdvUMlYT6U0rXwbg1Wtg2FnQtHXp8sJ8Nt57Ah02zSl31Z/mX8Z+Iyby65MHcW5GY86tQbgiIiJS9ygZq6lT74MXpla+3M094OcroUX4sPEd6+GOfnQoY9FnC8czrfA8Hrp4HH86qKwlREREpL5QMlZTw86KLxkDuK38G6q+UHQY1xZMIY8MZv/iKJa0jrPVTURERFKakrGaMoPBE2HhjCqven/h97iz8AfspikZaY3IvvEYmmfoIxEREWlI9Jc/EY65scrJ2EG5T1BEGgBvXT2e3h1aJCMyERERqeN0NWUitO4a96IfFh9M79wnKSKNqeMOYtXN31MiJiIi0oCpZSxR/ucTuGtohYtckn8pLxePAWDRDcfRoomqX0REpKFTy1iitO0JYy4pd/aEvNt5uXgMXds04/P/d4ISMREREQHUMpZYx90Eedth/uMlRW8WZXJRwdUU04hfnTiQCw+v6oOOREREpD5TMpZoJ98N37sT8ndw0oOLWPBV8FzKe84ZzolDu0QcnIiIiNQ1SsaSIa0x02dvKknEHj4/iwn9D4g4KBEREamLNGYsCd75NIfb/r0MgPvPPUSJmIiIiJRLyViCbdtdwHkPfwjA7acP49hBnSKOSEREROoyJWMJtDOvkFE3/QeAH47uwcRDukUckYiIiNR1SsYSpLComEv/Mp+8wmK+c3BH/u/UIVGHJCIiIilAA/gTIK+wiCv/+jFvLcth6riDuO74/lGHJCIiIilCyVgNrdiwg0ufzmbJuu1cd3x/po47KOqQREREJIUoGaum3flFPPXBF9z+2jKaZzTmoclZHDVAV02KiIhI1SgZi1NxsbN2227mrNrM+ys28fqS9WzdVcC4fh25beJQ9t+vadQhioiISApSMgZ8uWkXi9dtY/vuQrbnFrA9t5DtuwvYkVtIzs481mzexZqtu8kvLAagdbN0juzXkfPG9GRkr3YRRy8iIiKpTMkY8PqS9fz25cWlylo1acx+zdJp2yKd/p1bcczAA+jWrjnDu7dhYOf9aNTIIopWRERE6pNIkjEz+y5wF5AGPOjuN0cRxx4nD+vCmAPbs1+zxrRqmk7LJo1JU7IlIiIitaDWkzEzSwOmA8cAa4A5ZvaSuy+ueM3k6diqCR1bNYlq9yIiItKARXHT11HACndf6e75wDPAKRHEISIiIhK5KJKxrsDqmPdrwjIRERGRBqfOPg7JzKaY2Vwzm5uTkxN1OCIiIiJJEUUy9hXQPeZ9t7CsFHe/392z3D2rY8eOtRaciIiISG2KIhmbA/Q1s95mlgGcBbwUQRwiIiIikav1qyndvdDMLgH+TXBri4fdfVFtxyEiIiJSF0RynzF3fwV4JYp9i4iIiNQldXYAv4iIiEhDoGRMREREJEJKxkREREQiZO4edQyVMrMc4Isk76YDsDHJ+6gvVFfxU13FR/UUP9VVfFRP8VNdxS/euurp7nHflyslkrHaYGZz3T0r6jhSgeoqfqqr+Kie4qe6io/qKX6qq/glq67UTSkiIiISISVjIiIiIhFSMvat+6MOIIWoruKnuoqP6il+qqv4qJ7ip7qKX1LqSmPGRERERCKkljERERGRCCkZA8zsu2a2zMxWmNl1UccTBTNbZWYLzCzbzOaGZe3M7HUzWx7+bBuWm5n9MayvT8xsRMx2JofLLzezyVEdTyKZ2cNmtsHMFsaUJaxuzOyQsO5XhOta7R5hYpRTT9PM7KvwvMo2sxNi5v0iPOZlZnZcTHmZ30cz621mH4Tlz5pZRu0dXWKZWXcze8vMFpvZIjP7n7Bc51WMCupJ59VezKypmX1oZh+HdXVDWF7m8ZlZk/D9inB+r5htVakOU00FdfWomX0ec15lhuXJ//65e4OeCB5W/hlwIJABfAwMjDquCOphFdBhr7JbgevC19cBt4SvTwBeBQwYDXwQlrcDVoY/24av20Z9bAmomyOBEcDCZNQN8GG4rIXrHh/1MSewnqYBV5ex7MDwu9YE6B1+B9Mq+j4CfwXOCl/fB/wk6mOuQV11BkaEr1sBn4Z1ovMqvnrSebXvsRvQMnydDnwQfv5lHh/wU+C+8PVZwLPVrcNUmyqoq0eBiWUsn/Tvn1rGYBSwwt1Xuns+8AxwSsQx1RWnAI+Frx8DTo0pf9wDs4E2ZtYZOA543d03u/sW4HXgu7Ucc8K5+zvA5r2KE1I34bz93H22B9/gx2O2lVLKqafynAI84+557v45sILgu1jm9zH8r3ICMCNcP7bOU467r3P3+eHrHcASoCs6r0qpoJ7K02DPq/Dc2Bm+TQ8np/zjiz3XZgBHhfVRpTpM7lElRwV1VZ6kf/+UjAVf7NUx79dQ8Ze9vnLgNTObZ2ZTwrID3H1d+Ppr4IDwdXl11pDqMlF10zV8vXd5fXJJ2LT/8J5uN6peT+2Bre5euFd5ygu7h4YT/Heu86oce9UT6Lzah5mlmVk2sIEgMfiM8o+vpE7C+dsI6qNB/H7fu67cfc95dVN4Xt1pZk3CsqR//5SMyR6Hu/sI4HjgZ2Z2ZOzMMLvXpbdlUN1U6F7gICATWAfcEWk0dYyZtQSeAy539+2x83RefauMetJ5VQZ3L3L3TKAbQUtW/2gjqrv2riszGwz8gqDORhJ0PV5bW/EoGYOvgO4x77uFZQ2Ku38V/twAPE/wRV4fNrcS/twQLl5enTWkukxU3XwVvt67vF5w9/XhL71i4AGC8wqqXk+bCLoGGu9VnrLMLJ0gwXjK3f8eFuu82ktZ9aTzqmLuvhV4CxhD+cdXUifh/NYE9dGgfr/H1NV3w25xd/c84BGqf15V+funZAzmAH3DK04yCAYyvhRxTLXKzFqYWas9r4FjgYUE9bDn6pDJwIvh65eA88IrTEYD28KulX8Dx5pZ27Db4NiwrD5KSN2E87ab2ehwvMZ5MdtKeXsSi9D3Cc4rCOrprPCKrt5AX4IBr2V+H8NWoreAieH6sXWecsLP+iFgibv/PmaWzqsY5dWTzqt9mVlHM2sTvm4GHEMwxq6844s91yYCb4b1UaU6TPqBJUE5dbU05h8hIxjjFXteJff7V9ao/oY2EVwp8SlB//r1UccTwfEfSHBlzMfAoj11QDB+4A1gOfAfoF1YbsD0sL4WAFkx2/oRwYDPFcAFUR9bgurnaYKukAKCvv8LE1k3QFb4pf8MuIfwZsypNpVTT0+E9fBJ+Autc8zy14fHvIyYK43K+z6G5+mHYf39DWgS9THXoK4OJ+iC/ATIDqcTdF7FXU86r/atq6HAR2GdLAR+XdHxAU3D9yvC+QdWtw5Tbaqgrt4Mz6uFwJN8e8Vl0r9/ugO/iIiISITUTSkiIiISISVjIiIiIhFSMiYiIiISISVjIiIiIhFSMiYiIiISISVjIlKnmdn74c9eZnZOgrf9y7L2JSJSm3RrCxFJCWY2Hrja3U+swjqN/dvn8pU1f6e7t0xAeCIi1aaWMRGp08xsZ/jyZuAIM8s2syvCB/3eZmZzwgf7/jhcfryZvWtmLwGLw7IXzGyemS0ysylh2c1As3B7T8XuK7zT9m1mttDMFpjZmTHbnmlmM8xsqZk9Fd5hGzO72cwWh7HcXpt1JCKprXHli4iI1AnXEdMyFiZV29x9pJk1Ad4zs9fCZUcAg9398/D9j9x9c/jokzlm9py7X2dml3jwsOC9nUbwEOphQIdwnXfCecOBQcBa4D1grJktIXgsT3939z2PWhERiYdaxkQkVR1L8Ly4bOADgkcJ9Q3nfRiTiAFcZmYfA7MJHuzbl4odDjztwcOo1wNvAyNjtr3Gg4dUZwO9gG1ALvCQmZ0G7KrhsYlIA6JkTERSlQGXuntmOPV29z0tY9+ULBSMNTsaGOPuwwieSde0BvvNi3ldBOwZlzYKmAGcCPyrBtsXkQZGyZiIpIodQKuY9/8GfmJm6QBm1s/MWpSxXmtgi7vvMrP+wOiYeQV71t/Lu8CZ4bi0jsCRBA9TLpOZtQRau/srwBUE3ZsiInHRmDERSRWfAEVhd+OjwF0EXYTzw0H0OcCpZaz3L2BqOK5rGUFX5R73A5+Y2Xx3nxRT/jwwBvgYcOAad/86TObK0gp40cyaErTYXVmtIxSRBkm3thARERGJkLopRURERCKkZExEREQkQkrGRERERCKkZExEREQkQkrGRERERCKkZExEREQkQkrGRERERCKkZExEREQkQv8ff84pd2gnrcAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Visualize(gen_losses,disc_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,noise_dim,gen_features,SelfAttentionLayer):\n",
    "        super(Generator,self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(noise_dim      ,gen_features , kernel_size = 4, stride=2, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features  ,gen_features  , kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features  ,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            SelfAttentionLayer(gen_features*2,gen_features*2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*2, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            SelfAttentionLayer(gen_features*2,gen_features*2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.ConvTranspose1d(gen_features*2,gen_features*3, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(gen_features*3        ,gen_features*3, kernel_size = 4, stride=1, padding = 0)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.parametrizations.spectral_norm(nn.Conv1d(gen_features*3         ,gen_features*3, kernel_size = 4, stride=1, padding = 1)),\n",
    "            nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        return self.gen(x.type(torch.FloatTensor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,seq_len,disc_features,SelfAttentionLayer):\n",
    "        super(Critic,self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv1d(seq_len         , disc_features   , kernel_size = 3, stride = 1, padding = 1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(disc_features   , disc_features   , kernel_size = 3, stride = 1, padding = 1),torch.nn.InstanceNorm1d(disc_features ,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(disc_features   , disc_features   , kernel_size = 3, stride = 1, padding = 1),torch.nn.InstanceNorm1d(disc_features ,affine = True),nn.LeakyReLU(0.2),\n",
    "            SelfAttentionLayer(disc_features,disc_features),\n",
    "            nn.Conv1d(disc_features   , disc_features*2 , kernel_size = 3, stride = 1, padding = 1),torch.nn.InstanceNorm1d(disc_features*2,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(disc_features*2 , disc_features*2 , kernel_size = 3, stride = 1, padding = 1),torch.nn.InstanceNorm1d(disc_features*2,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(disc_features*2 , disc_features*2 , kernel_size = 3, stride = 1, padding = 1),torch.nn.InstanceNorm1d(disc_features*2,affine = True),nn.LeakyReLU(0.2),\n",
    "            SelfAttentionLayer(disc_features*2,disc_features*2),\n",
    "            nn.Conv1d(disc_features*2 , disc_features*4 , kernel_size = 3, stride = 2, padding = 1),torch.nn.InstanceNorm1d(disc_features*4,affine = True),nn.LeakyReLU(0.2),\n",
    "            # nn.Conv1d(disc_features*4 , disc_features*4 , kernel_size = 3, stride = 2, padding = 0),torch.nn.InstanceNorm1d(disc_features*4,affine = True),nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(disc_features*4 , 1               , kernel_size = 3, stride = 2, padding = 1),)\n",
    "    def forward(self,x):\n",
    "        return self.disc(x.type(torch.FloatTensor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Penalty (GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientPenalty(critic,real,fake,device):\n",
    "    Batch_size, seq_len,seq_dim = real.shape\n",
    "    alpha = torch.rand(Batch_size,1,1).repeat(1,seq_len,seq_dim).to(device)\n",
    "    interpolated_sequence = real * alpha + fake * (1 - alpha)\n",
    "    interpolated_sequence = interpolated_sequence.to(device)\n",
    "    # Batch_size = real.size(0)\n",
    "    # alpha = torch.rand(Batch_size,1,1).to(device)\n",
    "    # alpha = alpha.expand_as(real)\n",
    "    # interpolated_sequence = alpha * real + (1 - alpha) * fake\n",
    "   \n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_sequence)\n",
    "\n",
    "    # Take the gradient of the scires with respect to the sequences\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_sequence,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        retain_graph= True,\n",
    "        create_graph= True,\n",
    "        )[0]\n",
    "    #flattening\n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "\n",
    "    #l2 = 2\n",
    "    gradient_norm = gradient.norm(2,dim = 1)\n",
    "    # gradient_penalty = ((torch.sqrt(torch.sum(gradient ** 2, dim=1) + 1e-12) - 1) ** 2).mean()\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Batch 16/34                   Loss D: 4831.92, loss G: -0.06\n",
      "Epoch [1/500] Batch 31/34                   Loss D: 390.85, loss G: 0.10\n",
      "Epoch [2/500] Batch 16/34                   Loss D: 107.30, loss G: 0.12\n",
      "Epoch [2/500] Batch 31/34                   Loss D: 471.90, loss G: 0.12\n",
      "Epoch [3/500] Batch 16/34                   Loss D: 120.64, loss G: 0.12\n",
      "Epoch [3/500] Batch 31/34                   Loss D: 9.20, loss G: 0.12\n",
      "Epoch [4/500] Batch 16/34                   Loss D: 13.47, loss G: 0.12\n",
      "Epoch [4/500] Batch 31/34                   Loss D: 9.85, loss G: 0.12\n",
      "Epoch [5/500] Batch 16/34                   Loss D: 7.05, loss G: 0.12\n",
      "Epoch [5/500] Batch 31/34                   Loss D: 9.81, loss G: 0.12\n",
      "Epoch [6/500] Batch 16/34                   Loss D: 9.84, loss G: -0.03\n",
      "Epoch [6/500] Batch 31/34                   Loss D: 10.18, loss G: -0.41\n",
      "Epoch [7/500] Batch 16/34                   Loss D: 144.37, loss G: -0.43\n",
      "Epoch [7/500] Batch 31/34                   Loss D: 8.53, loss G: -0.43\n",
      "Epoch [8/500] Batch 16/34                   Loss D: 9.28, loss G: -0.43\n",
      "Epoch [8/500] Batch 31/34                   Loss D: 9.43, loss G: -0.43\n",
      "Epoch [9/500] Batch 16/34                   Loss D: 6.11, loss G: -0.43\n",
      "Epoch [9/500] Batch 31/34                   Loss D: 9.98, loss G: -0.43\n",
      "Epoch [10/500] Batch 16/34                   Loss D: 10.39, loss G: -0.43\n",
      "Epoch [10/500] Batch 31/34                   Loss D: 9.98, loss G: -0.43\n",
      "Epoch [11/500] Batch 16/34                   Loss D: 9.98, loss G: -0.61\n",
      "Epoch [11/500] Batch 31/34                   Loss D: 9.94, loss G: -0.35\n",
      "Epoch [12/500] Batch 16/34                   Loss D: 9.54, loss G: -0.48\n",
      "Epoch [12/500] Batch 31/34                   Loss D: 9.21, loss G: -0.48\n",
      "Epoch [13/500] Batch 16/34                   Loss D: 9.72, loss G: -0.48\n",
      "Epoch [13/500] Batch 31/34                   Loss D: 7.66, loss G: -0.48\n",
      "Epoch [14/500] Batch 16/34                   Loss D: 6.93, loss G: -0.48\n",
      "Epoch [14/500] Batch 31/34                   Loss D: 8.66, loss G: -0.48\n",
      "Epoch [15/500] Batch 16/34                   Loss D: 75.05, loss G: -0.48\n",
      "Epoch [15/500] Batch 31/34                   Loss D: 14.51, loss G: -0.48\n",
      "Epoch [16/500] Batch 16/34                   Loss D: 10.30, loss G: -0.37\n",
      "Epoch [16/500] Batch 31/34                   Loss D: 12.84, loss G: 0.08\n",
      "Epoch [17/500] Batch 16/34                   Loss D: 9.95, loss G: 0.01\n",
      "Epoch [17/500] Batch 31/34                   Loss D: 9.53, loss G: 0.01\n",
      "Epoch [18/500] Batch 16/34                   Loss D: 11.21, loss G: 0.01\n",
      "Epoch [18/500] Batch 31/34                   Loss D: 7.49, loss G: 0.01\n",
      "Epoch [19/500] Batch 16/34                   Loss D: 8.56, loss G: 0.01\n",
      "Epoch [19/500] Batch 31/34                   Loss D: 9.92, loss G: 0.01\n",
      "Epoch [20/500] Batch 16/34                   Loss D: 10.00, loss G: 0.01\n",
      "Epoch [20/500] Batch 31/34                   Loss D: 9.48, loss G: 0.01\n",
      "Epoch [21/500] Batch 16/34                   Loss D: 7.59, loss G: 2.19\n",
      "Epoch [21/500] Batch 31/34                   Loss D: 8.96, loss G: 2.01\n",
      "Epoch [22/500] Batch 16/34                   Loss D: 9.61, loss G: 2.10\n",
      "Epoch [22/500] Batch 31/34                   Loss D: 10.18, loss G: 2.10\n",
      "Epoch [23/500] Batch 16/34                   Loss D: 9.16, loss G: 2.10\n",
      "Epoch [23/500] Batch 31/34                   Loss D: 9.99, loss G: 2.10\n",
      "Epoch [24/500] Batch 16/34                   Loss D: 12.36, loss G: 2.10\n",
      "Epoch [24/500] Batch 31/34                   Loss D: 8.73, loss G: 2.10\n",
      "Epoch [25/500] Batch 16/34                   Loss D: 9.96, loss G: 2.10\n",
      "Epoch [25/500] Batch 31/34                   Loss D: 9.46, loss G: 2.10\n",
      "Epoch [26/500] Batch 16/34                   Loss D: 10.01, loss G: 1.78\n",
      "Epoch [26/500] Batch 31/34                   Loss D: 10.03, loss G: 1.57\n",
      "Epoch [27/500] Batch 16/34                   Loss D: 9.95, loss G: 1.98\n",
      "Epoch [27/500] Batch 31/34                   Loss D: 9.93, loss G: 1.98\n",
      "Epoch [28/500] Batch 16/34                   Loss D: 8.96, loss G: 1.98\n",
      "Epoch [28/500] Batch 31/34                   Loss D: 436.17, loss G: 1.98\n",
      "Epoch [29/500] Batch 16/34                   Loss D: 107.90, loss G: 1.98\n",
      "Epoch [29/500] Batch 31/34                   Loss D: 49.17, loss G: 1.98\n",
      "Epoch [30/500] Batch 16/34                   Loss D: 10.28, loss G: 1.98\n",
      "Epoch [30/500] Batch 31/34                   Loss D: 9.62, loss G: 1.98\n",
      "Epoch [31/500] Batch 16/34                   Loss D: 11.21, loss G: -0.20\n",
      "Epoch [31/500] Batch 31/34                   Loss D: 14.67, loss G: -0.22\n",
      "Epoch [32/500] Batch 16/34                   Loss D: 9.33, loss G: -0.11\n",
      "Epoch [32/500] Batch 31/34                   Loss D: 9.47, loss G: -0.11\n",
      "Epoch [33/500] Batch 16/34                   Loss D: 9.17, loss G: -0.11\n",
      "Epoch [33/500] Batch 31/34                   Loss D: 9.95, loss G: -0.11\n",
      "Epoch [34/500] Batch 16/34                   Loss D: 13.98, loss G: -0.11\n",
      "Epoch [34/500] Batch 31/34                   Loss D: 45.38, loss G: -0.11\n",
      "Epoch [35/500] Batch 16/34                   Loss D: 10.08, loss G: -0.11\n",
      "Epoch [35/500] Batch 31/34                   Loss D: 10.21, loss G: -0.11\n",
      "Epoch [36/500] Batch 16/34                   Loss D: 9.23, loss G: -1.25\n",
      "Epoch [36/500] Batch 31/34                   Loss D: 9.52, loss G: -1.39\n",
      "Epoch [37/500] Batch 16/34                   Loss D: 9.98, loss G: -0.94\n",
      "Epoch [37/500] Batch 31/34                   Loss D: 8.79, loss G: -0.94\n",
      "Epoch [38/500] Batch 16/34                   Loss D: 9.61, loss G: -0.94\n",
      "Epoch [38/500] Batch 31/34                   Loss D: 9.10, loss G: -0.94\n",
      "Epoch [39/500] Batch 16/34                   Loss D: 9.52, loss G: -0.94\n",
      "Epoch [39/500] Batch 31/34                   Loss D: 9.68, loss G: -0.94\n",
      "Epoch [40/500] Batch 16/34                   Loss D: 9.51, loss G: -0.94\n",
      "Epoch [40/500] Batch 31/34                   Loss D: 9.82, loss G: -0.94\n",
      "Epoch [41/500] Batch 16/34                   Loss D: 9.95, loss G: -1.58\n",
      "Epoch [41/500] Batch 31/34                   Loss D: 17.52, loss G: -0.95\n",
      "Epoch [42/500] Batch 16/34                   Loss D: 9.93, loss G: -1.02\n",
      "Epoch [42/500] Batch 31/34                   Loss D: 9.96, loss G: -1.02\n",
      "Epoch [43/500] Batch 16/34                   Loss D: 8.34, loss G: -1.02\n",
      "Epoch [43/500] Batch 31/34                   Loss D: 9.99, loss G: -1.02\n",
      "Epoch [44/500] Batch 16/34                   Loss D: 10.00, loss G: -1.02\n",
      "Epoch [44/500] Batch 31/34                   Loss D: 9.03, loss G: -1.02\n",
      "Epoch [45/500] Batch 16/34                   Loss D: 337.11, loss G: -1.02\n",
      "Epoch [45/500] Batch 31/34                   Loss D: 9.94, loss G: -1.02\n",
      "Epoch [46/500] Batch 16/34                   Loss D: 14.37, loss G: -1.42\n",
      "Epoch [46/500] Batch 31/34                   Loss D: 9.64, loss G: -1.32\n",
      "Epoch [47/500] Batch 16/34                   Loss D: 10.29, loss G: -2.02\n",
      "Epoch [47/500] Batch 31/34                   Loss D: 27.38, loss G: -2.02\n",
      "Epoch [48/500] Batch 16/34                   Loss D: 95.09, loss G: -2.02\n",
      "Epoch [48/500] Batch 31/34                   Loss D: 10.47, loss G: -2.02\n",
      "Epoch [49/500] Batch 16/34                   Loss D: 9.44, loss G: -2.02\n",
      "Epoch [49/500] Batch 31/34                   Loss D: 10.38, loss G: -2.02\n",
      "Epoch [50/500] Batch 16/34                   Loss D: 9.98, loss G: -2.02\n",
      "Epoch [50/500] Batch 31/34                   Loss D: 9.95, loss G: -2.02\n",
      "Epoch [51/500] Batch 16/34                   Loss D: 9.99, loss G: 0.08\n",
      "Epoch [51/500] Batch 31/34                   Loss D: 87.82, loss G: -0.14\n",
      "Epoch [52/500] Batch 16/34                   Loss D: 9.99, loss G: -0.98\n",
      "Epoch [52/500] Batch 31/34                   Loss D: 9.95, loss G: -0.98\n",
      "Epoch [53/500] Batch 16/34                   Loss D: 9.62, loss G: -0.98\n",
      "Epoch [53/500] Batch 31/34                   Loss D: 9.92, loss G: -0.98\n",
      "Epoch [54/500] Batch 16/34                   Loss D: 10.66, loss G: -0.98\n",
      "Epoch [54/500] Batch 31/34                   Loss D: 9.11, loss G: -0.98\n",
      "Epoch [55/500] Batch 16/34                   Loss D: 11.90, loss G: -0.98\n",
      "Epoch [55/500] Batch 31/34                   Loss D: 21.14, loss G: -0.98\n",
      "Epoch [56/500] Batch 16/34                   Loss D: 9.81, loss G: 1.64\n",
      "Epoch [56/500] Batch 31/34                   Loss D: 9.43, loss G: 1.60\n",
      "Epoch [57/500] Batch 16/34                   Loss D: 9.84, loss G: 1.78\n",
      "Epoch [57/500] Batch 31/34                   Loss D: 7.36, loss G: 1.78\n",
      "Epoch [58/500] Batch 16/34                   Loss D: 9.59, loss G: 1.78\n",
      "Epoch [58/500] Batch 31/34                   Loss D: 10.56, loss G: 1.78\n",
      "Epoch [59/500] Batch 16/34                   Loss D: 9.71, loss G: 1.78\n",
      "Epoch [59/500] Batch 31/34                   Loss D: 9.82, loss G: 1.78\n",
      "Epoch [60/500] Batch 16/34                   Loss D: 9.60, loss G: 1.78\n",
      "Epoch [60/500] Batch 31/34                   Loss D: 8.68, loss G: 1.78\n",
      "Epoch [61/500] Batch 16/34                   Loss D: 10.00, loss G: 2.95\n",
      "Epoch [61/500] Batch 31/34                   Loss D: 9.79, loss G: 2.88\n",
      "Epoch [62/500] Batch 16/34                   Loss D: 10.00, loss G: 1.66\n",
      "Epoch [62/500] Batch 31/34                   Loss D: 9.96, loss G: 1.66\n",
      "Epoch [63/500] Batch 16/34                   Loss D: 9.96, loss G: 1.66\n",
      "Epoch [63/500] Batch 31/34                   Loss D: 12.16, loss G: 1.66\n",
      "Epoch [64/500] Batch 16/34                   Loss D: 9.88, loss G: 1.66\n",
      "Epoch [64/500] Batch 31/34                   Loss D: 94.29, loss G: 1.66\n",
      "Epoch [65/500] Batch 16/34                   Loss D: 9.98, loss G: 1.66\n",
      "Epoch [65/500] Batch 31/34                   Loss D: 9.90, loss G: 1.66\n",
      "Epoch [66/500] Batch 16/34                   Loss D: 13.13, loss G: 3.13\n",
      "Epoch [66/500] Batch 31/34                   Loss D: 163.68, loss G: 3.62\n",
      "Epoch [67/500] Batch 16/34                   Loss D: 77.83, loss G: 2.58\n",
      "Epoch [67/500] Batch 31/34                   Loss D: 31.25, loss G: 2.58\n",
      "Epoch [68/500] Batch 16/34                   Loss D: 808.00, loss G: 2.58\n",
      "Epoch [68/500] Batch 31/34                   Loss D: 13.18, loss G: 2.58\n",
      "Epoch [69/500] Batch 16/34                   Loss D: 12.37, loss G: 2.58\n",
      "Epoch [69/500] Batch 31/34                   Loss D: 497.68, loss G: 2.58\n",
      "Epoch [70/500] Batch 16/34                   Loss D: 9.84, loss G: 2.58\n",
      "Epoch [70/500] Batch 31/34                   Loss D: 25.72, loss G: 2.58\n",
      "Epoch [71/500] Batch 16/34                   Loss D: 9.95, loss G: 4.82\n",
      "Epoch [71/500] Batch 31/34                   Loss D: 9.92, loss G: 5.29\n",
      "Epoch [72/500] Batch 16/34                   Loss D: 98.78, loss G: 4.83\n",
      "Epoch [72/500] Batch 31/34                   Loss D: 56.67, loss G: 4.83\n",
      "Epoch [73/500] Batch 16/34                   Loss D: 38.70, loss G: 4.83\n",
      "Epoch [73/500] Batch 31/34                   Loss D: 18.09, loss G: 4.83\n",
      "Epoch [74/500] Batch 16/34                   Loss D: 15.56, loss G: 4.83\n",
      "Epoch [74/500] Batch 31/34                   Loss D: 9.87, loss G: 4.83\n",
      "Epoch [75/500] Batch 16/34                   Loss D: 284.42, loss G: 4.83\n",
      "Epoch [75/500] Batch 31/34                   Loss D: 10.21, loss G: 4.83\n",
      "Epoch [76/500] Batch 16/34                   Loss D: 9.90, loss G: 5.05\n",
      "Epoch [76/500] Batch 31/34                   Loss D: 10.99, loss G: 4.88\n",
      "Epoch [77/500] Batch 16/34                   Loss D: 9.86, loss G: 4.08\n",
      "Epoch [77/500] Batch 31/34                   Loss D: 9.92, loss G: 4.08\n",
      "Epoch [78/500] Batch 16/34                   Loss D: 9.97, loss G: 4.08\n",
      "Epoch [78/500] Batch 31/34                   Loss D: 10.35, loss G: 4.08\n",
      "Epoch [79/500] Batch 16/34                   Loss D: 9.97, loss G: 4.08\n",
      "Epoch [79/500] Batch 31/34                   Loss D: 47.95, loss G: 4.08\n",
      "Epoch [80/500] Batch 16/34                   Loss D: 34.18, loss G: 4.08\n",
      "Epoch [80/500] Batch 31/34                   Loss D: 9.97, loss G: 4.08\n",
      "Epoch [81/500] Batch 16/34                   Loss D: 9.52, loss G: 5.69\n",
      "Epoch [81/500] Batch 31/34                   Loss D: 32.52, loss G: 5.94\n",
      "Epoch [82/500] Batch 16/34                   Loss D: 9.98, loss G: 4.73\n",
      "Epoch [82/500] Batch 31/34                   Loss D: 9.98, loss G: 4.73\n",
      "Epoch [83/500] Batch 16/34                   Loss D: 9.98, loss G: 4.73\n",
      "Epoch [83/500] Batch 31/34                   Loss D: 9.99, loss G: 4.73\n",
      "Epoch [84/500] Batch 16/34                   Loss D: 61.73, loss G: 4.73\n",
      "Epoch [84/500] Batch 31/34                   Loss D: 14.42, loss G: 4.73\n",
      "Epoch [85/500] Batch 16/34                   Loss D: 9.93, loss G: 4.73\n",
      "Epoch [85/500] Batch 31/34                   Loss D: 10.16, loss G: 4.73\n",
      "Epoch [86/500] Batch 16/34                   Loss D: 9.97, loss G: 6.66\n",
      "Epoch [86/500] Batch 31/34                   Loss D: 10.59, loss G: 6.83\n",
      "Epoch [87/500] Batch 16/34                   Loss D: 9.62, loss G: 5.42\n",
      "Epoch [87/500] Batch 31/34                   Loss D: 8.82, loss G: 5.42\n",
      "Epoch [88/500] Batch 16/34                   Loss D: 9.98, loss G: 5.42\n",
      "Epoch [88/500] Batch 31/34                   Loss D: 9.36, loss G: 5.42\n",
      "Epoch [89/500] Batch 16/34                   Loss D: 9.96, loss G: 5.42\n",
      "Epoch [89/500] Batch 31/34                   Loss D: 8.92, loss G: 5.42\n",
      "Epoch [90/500] Batch 16/34                   Loss D: 9.98, loss G: 5.42\n",
      "Epoch [90/500] Batch 31/34                   Loss D: 9.94, loss G: 5.42\n",
      "Epoch [91/500] Batch 16/34                   Loss D: 10.00, loss G: 2.51\n",
      "Epoch [91/500] Batch 31/34                   Loss D: 9.20, loss G: 2.55\n",
      "Epoch [92/500] Batch 16/34                   Loss D: 9.83, loss G: -0.11\n",
      "Epoch [92/500] Batch 31/34                   Loss D: 9.95, loss G: -0.11\n",
      "Epoch [93/500] Batch 16/34                   Loss D: 10.00, loss G: -0.11\n",
      "Epoch [93/500] Batch 31/34                   Loss D: 10.00, loss G: -0.11\n",
      "Epoch [94/500] Batch 16/34                   Loss D: 9.82, loss G: -0.11\n",
      "Epoch [94/500] Batch 31/34                   Loss D: 13.82, loss G: -0.11\n",
      "Epoch [95/500] Batch 16/34                   Loss D: 9.99, loss G: -0.11\n",
      "Epoch [95/500] Batch 31/34                   Loss D: 11.31, loss G: -0.11\n",
      "Epoch [96/500] Batch 16/34                   Loss D: 9.28, loss G: 2.81\n",
      "Epoch [96/500] Batch 31/34                   Loss D: 7.85, loss G: 3.13\n",
      "Epoch [97/500] Batch 16/34                   Loss D: 10.00, loss G: -0.33\n",
      "Epoch [97/500] Batch 31/34                   Loss D: 10.00, loss G: -0.33\n",
      "Epoch [98/500] Batch 16/34                   Loss D: 9.97, loss G: -0.33\n",
      "Epoch [98/500] Batch 31/34                   Loss D: 8.62, loss G: -0.33\n",
      "Epoch [99/500] Batch 16/34                   Loss D: 9.97, loss G: -0.33\n",
      "Epoch [99/500] Batch 31/34                   Loss D: 7.44, loss G: -0.33\n",
      "Epoch [100/500] Batch 16/34                   Loss D: 11.34, loss G: -0.33\n",
      "Epoch [100/500] Batch 31/34                   Loss D: 10.56, loss G: -0.33\n",
      "Epoch [101/500] Batch 16/34                   Loss D: 9.99, loss G: -1.68\n",
      "Epoch [101/500] Batch 31/34                   Loss D: 10.00, loss G: -1.64\n",
      "Epoch [102/500] Batch 16/34                   Loss D: 10.21, loss G: -2.21\n",
      "Epoch [102/500] Batch 31/34                   Loss D: 9.21, loss G: -2.21\n",
      "Epoch [103/500] Batch 16/34                   Loss D: 9.54, loss G: -2.21\n",
      "Epoch [103/500] Batch 31/34                   Loss D: 9.97, loss G: -2.21\n",
      "Epoch [104/500] Batch 16/34                   Loss D: 9.44, loss G: -2.21\n",
      "Epoch [104/500] Batch 31/34                   Loss D: 9.83, loss G: -2.21\n",
      "Epoch [105/500] Batch 16/34                   Loss D: 9.90, loss G: -2.21\n",
      "Epoch [105/500] Batch 31/34                   Loss D: 10.66, loss G: -2.21\n",
      "Epoch [106/500] Batch 16/34                   Loss D: 17.80, loss G: -0.86\n",
      "Epoch [106/500] Batch 31/34                   Loss D: 11.62, loss G: -0.48\n",
      "Epoch [107/500] Batch 16/34                   Loss D: 9.76, loss G: -0.86\n",
      "Epoch [107/500] Batch 31/34                   Loss D: 9.95, loss G: -0.86\n",
      "Epoch [108/500] Batch 16/34                   Loss D: 9.99, loss G: -0.86\n",
      "Epoch [108/500] Batch 31/34                   Loss D: 9.97, loss G: -0.86\n",
      "Epoch [109/500] Batch 16/34                   Loss D: 10.00, loss G: -0.86\n",
      "Epoch [109/500] Batch 31/34                   Loss D: 9.99, loss G: -0.86\n",
      "Epoch [110/500] Batch 16/34                   Loss D: 9.96, loss G: -0.86\n",
      "Epoch [110/500] Batch 31/34                   Loss D: 10.00, loss G: -0.86\n",
      "Epoch [111/500] Batch 16/34                   Loss D: 9.86, loss G: -3.09\n",
      "Epoch [111/500] Batch 31/34                   Loss D: 11.62, loss G: -0.41\n",
      "Epoch [112/500] Batch 16/34                   Loss D: 20.74, loss G: -1.50\n",
      "Epoch [112/500] Batch 31/34                   Loss D: 2628.90, loss G: -1.50\n",
      "Epoch [113/500] Batch 16/34                   Loss D: 267.05, loss G: -1.50\n",
      "Epoch [113/500] Batch 31/34                   Loss D: 48.10, loss G: -1.50\n",
      "Epoch [114/500] Batch 16/34                   Loss D: 33.19, loss G: -1.50\n",
      "Epoch [114/500] Batch 31/34                   Loss D: 668.53, loss G: -1.50\n",
      "Epoch [115/500] Batch 16/34                   Loss D: 80.25, loss G: -1.50\n",
      "Epoch [115/500] Batch 31/34                   Loss D: 30.44, loss G: -1.50\n",
      "Epoch [116/500] Batch 16/34                   Loss D: 16.46, loss G: -2.49\n",
      "Epoch [116/500] Batch 31/34                   Loss D: 11.53, loss G: -1.92\n",
      "Epoch [117/500] Batch 16/34                   Loss D: 11.83, loss G: -1.80\n",
      "Epoch [117/500] Batch 31/34                   Loss D: 9.87, loss G: -1.80\n",
      "Epoch [118/500] Batch 16/34                   Loss D: 9.96, loss G: -1.80\n",
      "Epoch [118/500] Batch 31/34                   Loss D: 10.00, loss G: -1.80\n",
      "Epoch [119/500] Batch 16/34                   Loss D: 11.47, loss G: -1.80\n",
      "Epoch [119/500] Batch 31/34                   Loss D: 9.72, loss G: -1.80\n",
      "Epoch [120/500] Batch 16/34                   Loss D: 10.00, loss G: -1.80\n",
      "Epoch [120/500] Batch 31/34                   Loss D: 9.81, loss G: -1.80\n",
      "Epoch [121/500] Batch 16/34                   Loss D: 9.99, loss G: -0.09\n",
      "Epoch [121/500] Batch 31/34                   Loss D: 9.98, loss G: -1.20\n",
      "Epoch [122/500] Batch 16/34                   Loss D: 11.53, loss G: -2.19\n",
      "Epoch [122/500] Batch 31/34                   Loss D: 10.00, loss G: -2.19\n",
      "Epoch [123/500] Batch 16/34                   Loss D: 12.79, loss G: -2.19\n",
      "Epoch [123/500] Batch 31/34                   Loss D: 11.73, loss G: -2.19\n",
      "Epoch [124/500] Batch 16/34                   Loss D: 9.90, loss G: -2.19\n",
      "Epoch [124/500] Batch 31/34                   Loss D: 10.86, loss G: -2.19\n",
      "Epoch [125/500] Batch 16/34                   Loss D: 9.95, loss G: -2.19\n",
      "Epoch [125/500] Batch 31/34                   Loss D: 10.06, loss G: -2.19\n",
      "Epoch [126/500] Batch 16/34                   Loss D: 11.51, loss G: -3.69\n",
      "Epoch [126/500] Batch 31/34                   Loss D: 20.00, loss G: -4.57\n",
      "Epoch [127/500] Batch 16/34                   Loss D: 8.90, loss G: -3.32\n",
      "Epoch [127/500] Batch 31/34                   Loss D: 10.00, loss G: -3.32\n",
      "Epoch [128/500] Batch 16/34                   Loss D: 9.85, loss G: -3.32\n",
      "Epoch [128/500] Batch 31/34                   Loss D: 13.04, loss G: -3.32\n",
      "Epoch [129/500] Batch 16/34                   Loss D: 9.32, loss G: -3.32\n",
      "Epoch [129/500] Batch 31/34                   Loss D: 10.27, loss G: -3.32\n",
      "Epoch [130/500] Batch 16/34                   Loss D: 9.85, loss G: -3.32\n",
      "Epoch [130/500] Batch 31/34                   Loss D: 10.00, loss G: -3.32\n",
      "Epoch [131/500] Batch 16/34                   Loss D: 347.06, loss G: -3.10\n",
      "Epoch [131/500] Batch 31/34                   Loss D: 1073.97, loss G: -2.41\n",
      "Epoch [132/500] Batch 16/34                   Loss D: 928.70, loss G: -5.06\n",
      "Epoch [132/500] Batch 31/34                   Loss D: 4880.57, loss G: -5.06\n",
      "Epoch [133/500] Batch 16/34                   Loss D: 35038.27, loss G: -5.06\n",
      "Epoch [133/500] Batch 31/34                   Loss D: 47.29, loss G: -5.06\n",
      "Epoch [134/500] Batch 16/34                   Loss D: 22.36, loss G: -5.06\n",
      "Epoch [134/500] Batch 31/34                   Loss D: 11.86, loss G: -5.06\n",
      "Epoch [135/500] Batch 16/34                   Loss D: 175198.78, loss G: -5.06\n",
      "Epoch [135/500] Batch 31/34                   Loss D: 8.73, loss G: -5.06\n",
      "Epoch [136/500] Batch 16/34                   Loss D: 9.97, loss G: 0.65\n",
      "Epoch [136/500] Batch 31/34                   Loss D: 9.96, loss G: 1.00\n",
      "Epoch [137/500] Batch 16/34                   Loss D: 15.59, loss G: -0.58\n",
      "Epoch [137/500] Batch 31/34                   Loss D: 9.85, loss G: -0.58\n",
      "Epoch [138/500] Batch 16/34                   Loss D: 10.00, loss G: -0.58\n",
      "Epoch [138/500] Batch 31/34                   Loss D: 10.46, loss G: -0.58\n",
      "Epoch [139/500] Batch 16/34                   Loss D: 9.97, loss G: -0.58\n",
      "Epoch [139/500] Batch 31/34                   Loss D: 9.92, loss G: -0.58\n",
      "Epoch [140/500] Batch 16/34                   Loss D: 9.99, loss G: -0.58\n",
      "Epoch [140/500] Batch 31/34                   Loss D: 9.75, loss G: -0.58\n",
      "Epoch [141/500] Batch 16/34                   Loss D: 10.00, loss G: 3.55\n",
      "Epoch [141/500] Batch 31/34                   Loss D: 10.00, loss G: 3.67\n",
      "Epoch [142/500] Batch 16/34                   Loss D: 10.00, loss G: 0.84\n",
      "Epoch [142/500] Batch 31/34                   Loss D: 8.59, loss G: 0.84\n",
      "Epoch [143/500] Batch 16/34                   Loss D: 9.97, loss G: 0.84\n",
      "Epoch [143/500] Batch 31/34                   Loss D: 10.48, loss G: 0.84\n",
      "Epoch [144/500] Batch 16/34                   Loss D: 10.00, loss G: 0.84\n",
      "Epoch [144/500] Batch 31/34                   Loss D: 10.00, loss G: 0.84\n",
      "Epoch [145/500] Batch 16/34                   Loss D: 9.51, loss G: 0.84\n",
      "Epoch [145/500] Batch 31/34                   Loss D: 10.00, loss G: 0.84\n",
      "Epoch [146/500] Batch 16/34                   Loss D: 9.96, loss G: 2.33\n",
      "Epoch [146/500] Batch 31/34                   Loss D: 9.10, loss G: 1.99\n",
      "Epoch [147/500] Batch 16/34                   Loss D: 9.31, loss G: 1.07\n",
      "Epoch [147/500] Batch 31/34                   Loss D: 21.29, loss G: 1.07\n",
      "Epoch [148/500] Batch 16/34                   Loss D: 13.69, loss G: 1.07\n",
      "Epoch [148/500] Batch 31/34                   Loss D: 10.21, loss G: 1.07\n",
      "Epoch [149/500] Batch 16/34                   Loss D: 30.08, loss G: 1.07\n",
      "Epoch [149/500] Batch 31/34                   Loss D: 518.32, loss G: 1.07\n",
      "Epoch [150/500] Batch 16/34                   Loss D: 11.54, loss G: 1.07\n",
      "Epoch [150/500] Batch 31/34                   Loss D: 8.79, loss G: 1.07\n",
      "Epoch [151/500] Batch 16/34                   Loss D: 9.44, loss G: -6.41\n",
      "Epoch [151/500] Batch 31/34                   Loss D: 19.57, loss G: -6.01\n",
      "Epoch [152/500] Batch 16/34                   Loss D: 10.00, loss G: -3.85\n",
      "Epoch [152/500] Batch 31/34                   Loss D: 10.45, loss G: -3.85\n",
      "Epoch [153/500] Batch 16/34                   Loss D: 9.98, loss G: -3.85\n",
      "Epoch [153/500] Batch 31/34                   Loss D: 9.77, loss G: -3.85\n",
      "Epoch [154/500] Batch 16/34                   Loss D: 10.00, loss G: -3.85\n",
      "Epoch [154/500] Batch 31/34                   Loss D: 10.00, loss G: -3.85\n",
      "Epoch [155/500] Batch 16/34                   Loss D: 9.98, loss G: -3.85\n",
      "Epoch [155/500] Batch 31/34                   Loss D: 9.89, loss G: -3.85\n",
      "Epoch [156/500] Batch 16/34                   Loss D: 9.93, loss G: -4.25\n",
      "Epoch [156/500] Batch 31/34                   Loss D: 12.08, loss G: -4.38\n",
      "Epoch [157/500] Batch 16/34                   Loss D: 10.00, loss G: -1.59\n",
      "Epoch [157/500] Batch 31/34                   Loss D: 11.66, loss G: -1.59\n",
      "Epoch [158/500] Batch 16/34                   Loss D: 7.95, loss G: -1.59\n",
      "Epoch [158/500] Batch 31/34                   Loss D: 9.97, loss G: -1.59\n",
      "Epoch [159/500] Batch 16/34                   Loss D: 10.20, loss G: -1.59\n",
      "Epoch [159/500] Batch 31/34                   Loss D: 10.10, loss G: -1.59\n",
      "Epoch [160/500] Batch 16/34                   Loss D: 9.99, loss G: -1.59\n",
      "Epoch [160/500] Batch 31/34                   Loss D: 10.37, loss G: -1.59\n",
      "Epoch [161/500] Batch 16/34                   Loss D: 9.59, loss G: -0.80\n",
      "Epoch [161/500] Batch 31/34                   Loss D: 10.00, loss G: -0.55\n",
      "Epoch [162/500] Batch 16/34                   Loss D: 9.69, loss G: 0.77\n",
      "Epoch [162/500] Batch 31/34                   Loss D: 9.99, loss G: 0.77\n",
      "Epoch [163/500] Batch 16/34                   Loss D: 10.01, loss G: 0.77\n",
      "Epoch [163/500] Batch 31/34                   Loss D: 9.98, loss G: 0.77\n",
      "Epoch [164/500] Batch 16/34                   Loss D: 9.25, loss G: 0.77\n",
      "Epoch [164/500] Batch 31/34                   Loss D: 12.08, loss G: 0.77\n",
      "Epoch [165/500] Batch 16/34                   Loss D: 9.01, loss G: 0.77\n",
      "Epoch [165/500] Batch 31/34                   Loss D: 30.86, loss G: 0.77\n",
      "Epoch [166/500] Batch 16/34                   Loss D: 10.59, loss G: -3.17\n",
      "Epoch [166/500] Batch 31/34                   Loss D: 15.14, loss G: -1.27\n",
      "Epoch [167/500] Batch 16/34                   Loss D: 9.98, loss G: -0.77\n",
      "Epoch [167/500] Batch 31/34                   Loss D: 9.80, loss G: -0.77\n",
      "Epoch [168/500] Batch 16/34                   Loss D: 9.03, loss G: -0.77\n",
      "Epoch [168/500] Batch 31/34                   Loss D: 14.83, loss G: -0.77\n",
      "Epoch [169/500] Batch 16/34                   Loss D: 9.69, loss G: -0.77\n",
      "Epoch [169/500] Batch 31/34                   Loss D: 9.54, loss G: -0.77\n",
      "Epoch [170/500] Batch 16/34                   Loss D: 11.01, loss G: -0.77\n",
      "Epoch [170/500] Batch 31/34                   Loss D: 10.00, loss G: -0.77\n",
      "Epoch [171/500] Batch 16/34                   Loss D: 33.81, loss G: 0.21\n",
      "Epoch [171/500] Batch 31/34                   Loss D: 8.57, loss G: -0.38\n",
      "Epoch [172/500] Batch 16/34                   Loss D: 7.89, loss G: 0.16\n",
      "Epoch [172/500] Batch 31/34                   Loss D: 9.97, loss G: 0.16\n",
      "Epoch [173/500] Batch 16/34                   Loss D: 9.63, loss G: 0.16\n",
      "Epoch [173/500] Batch 31/34                   Loss D: 10.77, loss G: 0.16\n",
      "Epoch [174/500] Batch 16/34                   Loss D: 9.98, loss G: 0.16\n",
      "Epoch [174/500] Batch 31/34                   Loss D: 10.00, loss G: 0.16\n",
      "Epoch [175/500] Batch 16/34                   Loss D: 8.50, loss G: 0.16\n",
      "Epoch [175/500] Batch 31/34                   Loss D: 9.99, loss G: 0.16\n",
      "Epoch [176/500] Batch 16/34                   Loss D: 10.00, loss G: -1.50\n",
      "Epoch [176/500] Batch 31/34                   Loss D: 10.00, loss G: -1.92\n",
      "Epoch [177/500] Batch 16/34                   Loss D: 10.00, loss G: -0.78\n",
      "Epoch [177/500] Batch 31/34                   Loss D: 49.52, loss G: -0.78\n",
      "Epoch [178/500] Batch 16/34                   Loss D: 18.98, loss G: -0.78\n",
      "Epoch [178/500] Batch 31/34                   Loss D: 14.61, loss G: -0.78\n",
      "Epoch [179/500] Batch 16/34                   Loss D: 9.98, loss G: -0.78\n",
      "Epoch [179/500] Batch 31/34                   Loss D: 9.81, loss G: -0.78\n",
      "Epoch [180/500] Batch 16/34                   Loss D: 10.00, loss G: -0.78\n",
      "Epoch [180/500] Batch 31/34                   Loss D: 9.96, loss G: -0.78\n",
      "Epoch [181/500] Batch 16/34                   Loss D: 9.85, loss G: -5.57\n",
      "Epoch [181/500] Batch 31/34                   Loss D: 9.13, loss G: -5.23\n",
      "Epoch [182/500] Batch 16/34                   Loss D: 14.19, loss G: -2.56\n",
      "Epoch [182/500] Batch 31/34                   Loss D: 9.99, loss G: -2.56\n",
      "Epoch [183/500] Batch 16/34                   Loss D: 9.99, loss G: -2.56\n",
      "Epoch [183/500] Batch 31/34                   Loss D: 9.97, loss G: -2.56\n",
      "Epoch [184/500] Batch 16/34                   Loss D: 10.18, loss G: -2.56\n",
      "Epoch [184/500] Batch 31/34                   Loss D: 29.83, loss G: -2.56\n",
      "Epoch [185/500] Batch 16/34                   Loss D: 9.95, loss G: -2.56\n",
      "Epoch [185/500] Batch 31/34                   Loss D: 10.03, loss G: -2.56\n",
      "Epoch [186/500] Batch 16/34                   Loss D: 10.00, loss G: -2.53\n",
      "Epoch [186/500] Batch 31/34                   Loss D: 9.93, loss G: -2.37\n",
      "Epoch [187/500] Batch 16/34                   Loss D: 10.00, loss G: -1.87\n",
      "Epoch [187/500] Batch 31/34                   Loss D: 10.00, loss G: -1.87\n",
      "Epoch [188/500] Batch 16/34                   Loss D: 9.57, loss G: -1.87\n",
      "Epoch [188/500] Batch 31/34                   Loss D: 10.00, loss G: -1.87\n",
      "Epoch [189/500] Batch 16/34                   Loss D: 9.98, loss G: -1.87\n",
      "Epoch [189/500] Batch 31/34                   Loss D: 26.26, loss G: -1.87\n",
      "Epoch [190/500] Batch 16/34                   Loss D: 29.51, loss G: -1.87\n",
      "Epoch [190/500] Batch 31/34                   Loss D: 190.58, loss G: -1.87\n",
      "Epoch [191/500] Batch 16/34                   Loss D: 33850.70, loss G: -3.29\n",
      "Epoch [191/500] Batch 31/34                   Loss D: 18538.22, loss G: -4.72\n",
      "Epoch [192/500] Batch 16/34                   Loss D: 48672.79, loss G: -4.43\n",
      "Epoch [192/500] Batch 31/34                   Loss D: 34117.68, loss G: -4.43\n",
      "Epoch [193/500] Batch 16/34                   Loss D: 63835.64, loss G: -4.43\n",
      "Epoch [193/500] Batch 31/34                   Loss D: 20943.93, loss G: -4.43\n",
      "Epoch [194/500] Batch 16/34                   Loss D: 17299.74, loss G: -4.43\n",
      "Epoch [194/500] Batch 31/34                   Loss D: 2163.81, loss G: -4.43\n",
      "Epoch [195/500] Batch 16/34                   Loss D: 3935.68, loss G: -4.43\n",
      "Epoch [195/500] Batch 31/34                   Loss D: 3226.47, loss G: -4.43\n",
      "Epoch [196/500] Batch 16/34                   Loss D: 8842.04, loss G: -4.81\n",
      "Epoch [196/500] Batch 31/34                   Loss D: 423.33, loss G: -4.73\n",
      "Epoch [197/500] Batch 16/34                   Loss D: 1528.17, loss G: -4.52\n",
      "Epoch [197/500] Batch 31/34                   Loss D: 46139.52, loss G: -4.52\n",
      "Epoch [198/500] Batch 16/34                   Loss D: 3321.22, loss G: -4.52\n",
      "Epoch [198/500] Batch 31/34                   Loss D: 5007.88, loss G: -4.52\n",
      "Epoch [199/500] Batch 16/34                   Loss D: 185.69, loss G: -4.52\n",
      "Epoch [199/500] Batch 31/34                   Loss D: 2909.28, loss G: -4.52\n",
      "Epoch [200/500] Batch 16/34                   Loss D: 961.42, loss G: -4.52\n",
      "Epoch [200/500] Batch 31/34                   Loss D: 1123.81, loss G: -4.52\n",
      "Epoch [201/500] Batch 16/34                   Loss D: 354.71, loss G: -4.77\n",
      "Epoch [201/500] Batch 31/34                   Loss D: 442.04, loss G: -4.94\n",
      "Epoch [202/500] Batch 16/34                   Loss D: 148.44, loss G: -3.92\n",
      "Epoch [202/500] Batch 31/34                   Loss D: 2585.79, loss G: -3.92\n",
      "Epoch [203/500] Batch 16/34                   Loss D: 146.87, loss G: -3.92\n",
      "Epoch [203/500] Batch 31/34                   Loss D: 15692.00, loss G: -3.92\n",
      "Epoch [204/500] Batch 16/34                   Loss D: 90.55, loss G: -3.92\n",
      "Epoch [204/500] Batch 31/34                   Loss D: 250.43, loss G: -3.92\n",
      "Epoch [205/500] Batch 16/34                   Loss D: 439.79, loss G: -3.92\n",
      "Epoch [205/500] Batch 31/34                   Loss D: 1815.11, loss G: -3.92\n",
      "Epoch [206/500] Batch 16/34                   Loss D: 2506.55, loss G: -5.05\n",
      "Epoch [206/500] Batch 31/34                   Loss D: 28429.26, loss G: -5.06\n",
      "Epoch [207/500] Batch 16/34                   Loss D: 3538.62, loss G: -5.06\n",
      "Epoch [207/500] Batch 31/34                   Loss D: 1636.98, loss G: -5.06\n",
      "Epoch [208/500] Batch 16/34                   Loss D: 122.72, loss G: -5.06\n",
      "Epoch [208/500] Batch 31/34                   Loss D: 134.74, loss G: -5.06\n",
      "Epoch [209/500] Batch 16/34                   Loss D: 6350.39, loss G: -5.06\n",
      "Epoch [209/500] Batch 31/34                   Loss D: 6740.08, loss G: -5.06\n",
      "Epoch [210/500] Batch 16/34                   Loss D: 541.09, loss G: -5.06\n",
      "Epoch [210/500] Batch 31/34                   Loss D: 524.94, loss G: -5.06\n",
      "Epoch [211/500] Batch 16/34                   Loss D: 422752.56, loss G: -5.04\n",
      "Epoch [211/500] Batch 31/34                   Loss D: 1218.91, loss G: -5.07\n",
      "Epoch [212/500] Batch 16/34                   Loss D: 3820.11, loss G: -4.49\n",
      "Epoch [212/500] Batch 31/34                   Loss D: 6825.91, loss G: -4.49\n",
      "Epoch [213/500] Batch 16/34                   Loss D: 604.45, loss G: -4.49\n",
      "Epoch [213/500] Batch 31/34                   Loss D: 1993.41, loss G: -4.49\n",
      "Epoch [214/500] Batch 16/34                   Loss D: 104.08, loss G: -4.49\n",
      "Epoch [214/500] Batch 31/34                   Loss D: 278.42, loss G: -4.49\n",
      "Epoch [215/500] Batch 16/34                   Loss D: 287.90, loss G: -4.49\n",
      "Epoch [215/500] Batch 31/34                   Loss D: 162.27, loss G: -4.49\n",
      "Epoch [216/500] Batch 16/34                   Loss D: 78.59, loss G: -5.25\n",
      "Epoch [216/500] Batch 31/34                   Loss D: 94.01, loss G: -5.36\n",
      "Epoch [217/500] Batch 16/34                   Loss D: 19.66, loss G: -3.52\n",
      "Epoch [217/500] Batch 31/34                   Loss D: 9.88, loss G: -3.52\n",
      "Epoch [218/500] Batch 16/34                   Loss D: 23.48, loss G: -3.52\n",
      "Epoch [218/500] Batch 31/34                   Loss D: 15.91, loss G: -3.52\n",
      "Epoch [219/500] Batch 16/34                   Loss D: 26544.11, loss G: -3.52\n",
      "Epoch [219/500] Batch 31/34                   Loss D: 14655.12, loss G: -3.52\n",
      "Epoch [220/500] Batch 16/34                   Loss D: 35946.73, loss G: -3.52\n",
      "Epoch [220/500] Batch 31/34                   Loss D: 465862.69, loss G: -3.52\n",
      "Epoch [221/500] Batch 16/34                   Loss D: 29.80, loss G: -5.39\n",
      "Epoch [221/500] Batch 31/34                   Loss D: 1482.51, loss G: -5.39\n",
      "Epoch [222/500] Batch 16/34                   Loss D: 194778.34, loss G: -5.05\n",
      "Epoch [222/500] Batch 31/34                   Loss D: 2047.13, loss G: -5.05\n",
      "Epoch [223/500] Batch 16/34                   Loss D: 8456.65, loss G: -5.05\n",
      "Epoch [223/500] Batch 31/34                   Loss D: 6973.42, loss G: -5.05\n",
      "Epoch [224/500] Batch 16/34                   Loss D: 1468.42, loss G: -5.05\n",
      "Epoch [224/500] Batch 31/34                   Loss D: 110.78, loss G: -5.05\n",
      "Epoch [225/500] Batch 16/34                   Loss D: 39713.08, loss G: -5.05\n",
      "Epoch [225/500] Batch 31/34                   Loss D: 6655.80, loss G: -5.05\n",
      "Epoch [226/500] Batch 16/34                   Loss D: 308.96, loss G: -5.63\n",
      "Epoch [226/500] Batch 31/34                   Loss D: 12199.56, loss G: -5.52\n",
      "Epoch [227/500] Batch 16/34                   Loss D: 678.88, loss G: -4.86\n",
      "Epoch [227/500] Batch 31/34                   Loss D: 488.50, loss G: -4.86\n",
      "Epoch [228/500] Batch 16/34                   Loss D: 58543.38, loss G: -4.86\n",
      "Epoch [228/500] Batch 31/34                   Loss D: 779.40, loss G: -4.86\n",
      "Epoch [229/500] Batch 16/34                   Loss D: 33071.81, loss G: -4.86\n",
      "Epoch [229/500] Batch 31/34                   Loss D: 128.52, loss G: -4.86\n",
      "Epoch [230/500] Batch 16/34                   Loss D: 4430.90, loss G: -4.86\n",
      "Epoch [230/500] Batch 31/34                   Loss D: 30.12, loss G: -4.86\n",
      "Epoch [231/500] Batch 16/34                   Loss D: 24.64, loss G: -5.80\n",
      "Epoch [231/500] Batch 31/34                   Loss D: 30.03, loss G: -5.77\n",
      "Epoch [232/500] Batch 16/34                   Loss D: 1100.28, loss G: -5.00\n",
      "Epoch [232/500] Batch 31/34                   Loss D: 366.08, loss G: -5.00\n",
      "Epoch [233/500] Batch 16/34                   Loss D: 1055.27, loss G: -5.00\n",
      "Epoch [233/500] Batch 31/34                   Loss D: 288.50, loss G: -5.00\n",
      "Epoch [234/500] Batch 16/34                   Loss D: 22.27, loss G: -5.00\n",
      "Epoch [234/500] Batch 31/34                   Loss D: 4240.76, loss G: -5.00\n",
      "Epoch [235/500] Batch 16/34                   Loss D: 15.60, loss G: -5.00\n",
      "Epoch [235/500] Batch 31/34                   Loss D: 1115.35, loss G: -5.00\n",
      "Epoch [236/500] Batch 16/34                   Loss D: 4413.92, loss G: -4.78\n",
      "Epoch [236/500] Batch 31/34                   Loss D: 448.25, loss G: -4.75\n",
      "Epoch [237/500] Batch 16/34                   Loss D: 7183.44, loss G: -3.97\n",
      "Epoch [237/500] Batch 31/34                   Loss D: 529.44, loss G: -3.97\n",
      "Epoch [238/500] Batch 16/34                   Loss D: 84.87, loss G: -3.97\n",
      "Epoch [238/500] Batch 31/34                   Loss D: 97.93, loss G: -3.97\n",
      "Epoch [239/500] Batch 16/34                   Loss D: 16.27, loss G: -3.97\n",
      "Epoch [239/500] Batch 31/34                   Loss D: 5339.24, loss G: -3.97\n",
      "Epoch [240/500] Batch 16/34                   Loss D: 31.84, loss G: -3.97\n",
      "Epoch [240/500] Batch 31/34                   Loss D: 627.74, loss G: -3.97\n",
      "Epoch [241/500] Batch 16/34                   Loss D: 13920.21, loss G: -4.84\n",
      "Epoch [241/500] Batch 31/34                   Loss D: 174180.86, loss G: -4.87\n",
      "Epoch [242/500] Batch 16/34                   Loss D: 3281.69, loss G: -4.86\n",
      "Epoch [242/500] Batch 31/34                   Loss D: 68733.95, loss G: -4.86\n",
      "Epoch [243/500] Batch 16/34                   Loss D: 2588.94, loss G: -4.86\n",
      "Epoch [243/500] Batch 31/34                   Loss D: 4666.93, loss G: -4.86\n",
      "Epoch [244/500] Batch 16/34                   Loss D: 40.73, loss G: -4.86\n",
      "Epoch [244/500] Batch 31/34                   Loss D: 1807.71, loss G: -4.86\n",
      "Epoch [245/500] Batch 16/34                   Loss D: 350.73, loss G: -4.86\n",
      "Epoch [245/500] Batch 31/34                   Loss D: 387.32, loss G: -4.86\n",
      "Epoch [246/500] Batch 16/34                   Loss D: 1003897.19, loss G: -4.89\n",
      "Epoch [246/500] Batch 31/34                   Loss D: 4276.85, loss G: -4.62\n",
      "Epoch [247/500] Batch 16/34                   Loss D: 4333.43, loss G: -4.79\n",
      "Epoch [247/500] Batch 31/34                   Loss D: 44.27, loss G: -4.79\n",
      "Epoch [248/500] Batch 16/34                   Loss D: 638.95, loss G: -4.79\n",
      "Epoch [248/500] Batch 31/34                   Loss D: 1322.63, loss G: -4.79\n",
      "Epoch [249/500] Batch 16/34                   Loss D: 163.72, loss G: -4.79\n",
      "Epoch [249/500] Batch 31/34                   Loss D: 40.35, loss G: -4.79\n",
      "Epoch [250/500] Batch 16/34                   Loss D: 550.37, loss G: -4.79\n",
      "Epoch [250/500] Batch 31/34                   Loss D: 16520.35, loss G: -4.79\n",
      "Epoch [251/500] Batch 16/34                   Loss D: 60.93, loss G: -4.78\n",
      "Epoch [251/500] Batch 31/34                   Loss D: 661.56, loss G: -4.81\n",
      "Epoch [252/500] Batch 16/34                   Loss D: 207.26, loss G: -4.66\n",
      "Epoch [252/500] Batch 31/34                   Loss D: 575.54, loss G: -4.66\n",
      "Epoch [253/500] Batch 16/34                   Loss D: 196.12, loss G: -4.66\n",
      "Epoch [253/500] Batch 31/34                   Loss D: 34.60, loss G: -4.66\n",
      "Epoch [254/500] Batch 16/34                   Loss D: 112.23, loss G: -4.66\n",
      "Epoch [254/500] Batch 31/34                   Loss D: 9.93, loss G: -4.66\n",
      "Epoch [255/500] Batch 16/34                   Loss D: 13.63, loss G: -4.66\n",
      "Epoch [255/500] Batch 31/34                   Loss D: 9.85, loss G: -4.66\n",
      "Epoch [256/500] Batch 16/34                   Loss D: 65314.70, loss G: -4.84\n",
      "Epoch [256/500] Batch 31/34                   Loss D: 3211.11, loss G: -4.74\n",
      "Epoch [257/500] Batch 16/34                   Loss D: 14717.30, loss G: -4.19\n",
      "Epoch [257/500] Batch 31/34                   Loss D: 74534.17, loss G: -4.19\n",
      "Epoch [258/500] Batch 16/34                   Loss D: 117710.85, loss G: -4.19\n",
      "Epoch [258/500] Batch 31/34                   Loss D: 38113.52, loss G: -4.19\n",
      "Epoch [259/500] Batch 16/34                   Loss D: 38248.05, loss G: -4.19\n",
      "Epoch [259/500] Batch 31/34                   Loss D: 1738.04, loss G: -4.19\n",
      "Epoch [260/500] Batch 16/34                   Loss D: 25559.22, loss G: -4.19\n",
      "Epoch [260/500] Batch 31/34                   Loss D: 4459.54, loss G: -4.19\n",
      "Epoch [261/500] Batch 16/34                   Loss D: 551442.38, loss G: -4.88\n",
      "Epoch [261/500] Batch 31/34                   Loss D: 690.04, loss G: -4.93\n",
      "Epoch [262/500] Batch 16/34                   Loss D: 947319.44, loss G: -4.91\n",
      "Epoch [262/500] Batch 31/34                   Loss D: 80.42, loss G: -4.91\n",
      "Epoch [263/500] Batch 16/34                   Loss D: 1719.53, loss G: -4.91\n",
      "Epoch [263/500] Batch 31/34                   Loss D: 82.24, loss G: -4.91\n",
      "Epoch [264/500] Batch 16/34                   Loss D: 74.88, loss G: -4.91\n",
      "Epoch [264/500] Batch 31/34                   Loss D: 76.01, loss G: -4.91\n",
      "Epoch [265/500] Batch 16/34                   Loss D: 118.53, loss G: -4.91\n",
      "Epoch [265/500] Batch 31/34                   Loss D: 64.50, loss G: -4.91\n",
      "Epoch [266/500] Batch 16/34                   Loss D: 194.40, loss G: -4.93\n",
      "Epoch [266/500] Batch 31/34                   Loss D: 2811.27, loss G: -4.66\n",
      "Epoch [267/500] Batch 16/34                   Loss D: 14414.67, loss G: -4.76\n",
      "Epoch [267/500] Batch 31/34                   Loss D: 126.71, loss G: -4.76\n",
      "Epoch [268/500] Batch 16/34                   Loss D: 2303783.25, loss G: -4.76\n",
      "Epoch [268/500] Batch 31/34                   Loss D: 1159.03, loss G: -4.76\n",
      "Epoch [269/500] Batch 16/34                   Loss D: 2983.80, loss G: -4.76\n",
      "Epoch [269/500] Batch 31/34                   Loss D: 125.69, loss G: -4.76\n",
      "Epoch [270/500] Batch 16/34                   Loss D: 1841.39, loss G: -4.76\n",
      "Epoch [270/500] Batch 31/34                   Loss D: 186620.28, loss G: -4.76\n",
      "Epoch [271/500] Batch 16/34                   Loss D: 30823.45, loss G: -4.71\n",
      "Epoch [271/500] Batch 31/34                   Loss D: 573.25, loss G: -4.70\n",
      "Epoch [272/500] Batch 16/34                   Loss D: 378855.91, loss G: -4.70\n",
      "Epoch [272/500] Batch 31/34                   Loss D: 45.31, loss G: -4.70\n",
      "Epoch [273/500] Batch 16/34                   Loss D: 86.46, loss G: -4.70\n",
      "Epoch [273/500] Batch 31/34                   Loss D: 260.56, loss G: -4.70\n",
      "Epoch [274/500] Batch 16/34                   Loss D: 44.76, loss G: -4.70\n",
      "Epoch [274/500] Batch 31/34                   Loss D: 913.39, loss G: -4.70\n",
      "Epoch [275/500] Batch 16/34                   Loss D: 65.87, loss G: -4.70\n",
      "Epoch [275/500] Batch 31/34                   Loss D: 50207.61, loss G: -4.70\n",
      "Epoch [276/500] Batch 16/34                   Loss D: 5987.05, loss G: -4.44\n",
      "Epoch [276/500] Batch 31/34                   Loss D: 687.47, loss G: -4.27\n",
      "Epoch [277/500] Batch 16/34                   Loss D: 1805.85, loss G: -4.45\n",
      "Epoch [277/500] Batch 31/34                   Loss D: 3267.34, loss G: -4.45\n",
      "Epoch [278/500] Batch 16/34                   Loss D: 430.83, loss G: -4.45\n",
      "Epoch [278/500] Batch 31/34                   Loss D: 700.06, loss G: -4.45\n",
      "Epoch [279/500] Batch 16/34                   Loss D: 120173.57, loss G: -4.45\n",
      "Epoch [279/500] Batch 31/34                   Loss D: 12026.56, loss G: -4.45\n",
      "Epoch [280/500] Batch 16/34                   Loss D: 913.82, loss G: -4.45\n",
      "Epoch [280/500] Batch 31/34                   Loss D: 150.57, loss G: -4.45\n",
      "Epoch [281/500] Batch 16/34                   Loss D: 2729495.25, loss G: -3.93\n",
      "Epoch [281/500] Batch 31/34                   Loss D: 9652.33, loss G: -4.00\n",
      "Epoch [282/500] Batch 16/34                   Loss D: 3460.20, loss G: -4.28\n",
      "Epoch [282/500] Batch 31/34                   Loss D: 54.42, loss G: -4.28\n",
      "Epoch [283/500] Batch 16/34                   Loss D: 71.31, loss G: -4.28\n",
      "Epoch [283/500] Batch 31/34                   Loss D: 57.34, loss G: -4.28\n",
      "Epoch [284/500] Batch 16/34                   Loss D: 152.76, loss G: -4.28\n",
      "Epoch [284/500] Batch 31/34                   Loss D: 1694.77, loss G: -4.28\n",
      "Epoch [285/500] Batch 16/34                   Loss D: 483.02, loss G: -4.28\n",
      "Epoch [285/500] Batch 31/34                   Loss D: 42.70, loss G: -4.28\n",
      "Epoch [286/500] Batch 16/34                   Loss D: 151.26, loss G: -3.80\n",
      "Epoch [286/500] Batch 31/34                   Loss D: 45.46, loss G: -3.90\n",
      "Epoch [287/500] Batch 16/34                   Loss D: 22.80, loss G: -3.98\n",
      "Epoch [287/500] Batch 31/34                   Loss D: 657.65, loss G: -3.98\n",
      "Epoch [288/500] Batch 16/34                   Loss D: 316.01, loss G: -3.98\n",
      "Epoch [288/500] Batch 31/34                   Loss D: 31.09, loss G: -3.98\n",
      "Epoch [289/500] Batch 16/34                   Loss D: 240.84, loss G: -3.98\n",
      "Epoch [289/500] Batch 31/34                   Loss D: 188.92, loss G: -3.98\n",
      "Epoch [290/500] Batch 16/34                   Loss D: 4307.66, loss G: -3.98\n",
      "Epoch [290/500] Batch 31/34                   Loss D: 1160.87, loss G: -3.98\n",
      "Epoch [291/500] Batch 16/34                   Loss D: 10.59, loss G: -3.90\n",
      "Epoch [291/500] Batch 31/34                   Loss D: 9.77, loss G: -3.86\n",
      "Epoch [292/500] Batch 16/34                   Loss D: 20.14, loss G: -3.79\n",
      "Epoch [292/500] Batch 31/34                   Loss D: 10.97, loss G: -3.79\n",
      "Epoch [293/500] Batch 16/34                   Loss D: 16134.57, loss G: -3.79\n",
      "Epoch [293/500] Batch 31/34                   Loss D: 27.60, loss G: -3.79\n",
      "Epoch [294/500] Batch 16/34                   Loss D: 159.18, loss G: -3.79\n",
      "Epoch [294/500] Batch 31/34                   Loss D: 77.78, loss G: -3.79\n",
      "Epoch [295/500] Batch 16/34                   Loss D: 15.83, loss G: -3.79\n",
      "Epoch [295/500] Batch 31/34                   Loss D: 12.75, loss G: -3.79\n",
      "Epoch [296/500] Batch 16/34                   Loss D: 53.25, loss G: -3.59\n",
      "Epoch [296/500] Batch 31/34                   Loss D: 1937.39, loss G: -3.60\n",
      "Epoch [297/500] Batch 16/34                   Loss D: 59.92, loss G: -3.75\n",
      "Epoch [297/500] Batch 31/34                   Loss D: 29.48, loss G: -3.75\n",
      "Epoch [298/500] Batch 16/34                   Loss D: 63.10, loss G: -3.75\n",
      "Epoch [298/500] Batch 31/34                   Loss D: 639.26, loss G: -3.75\n",
      "Epoch [299/500] Batch 16/34                   Loss D: 46.32, loss G: -3.75\n",
      "Epoch [299/500] Batch 31/34                   Loss D: 13.34, loss G: -3.75\n",
      "Epoch [300/500] Batch 16/34                   Loss D: 24.02, loss G: -3.75\n",
      "Epoch [300/500] Batch 31/34                   Loss D: 117.07, loss G: -3.75\n",
      "Epoch [301/500] Batch 16/34                   Loss D: 62.13, loss G: -3.36\n",
      "Epoch [301/500] Batch 31/34                   Loss D: 10.40, loss G: -3.02\n",
      "Epoch [302/500] Batch 16/34                   Loss D: 9.78, loss G: -3.36\n",
      "Epoch [302/500] Batch 31/34                   Loss D: 169486.69, loss G: -3.36\n",
      "Epoch [303/500] Batch 16/34                   Loss D: 36.24, loss G: -3.36\n",
      "Epoch [303/500] Batch 31/34                   Loss D: 9.98, loss G: -3.36\n",
      "Epoch [304/500] Batch 16/34                   Loss D: 14.42, loss G: -3.36\n",
      "Epoch [304/500] Batch 31/34                   Loss D: 2183.09, loss G: -3.36\n",
      "Epoch [305/500] Batch 16/34                   Loss D: 10.04, loss G: -3.36\n",
      "Epoch [305/500] Batch 31/34                   Loss D: 10.07, loss G: -3.36\n",
      "Epoch [306/500] Batch 16/34                   Loss D: 9.95, loss G: -2.94\n",
      "Epoch [306/500] Batch 31/34                   Loss D: 9.96, loss G: -3.02\n",
      "Epoch [307/500] Batch 16/34                   Loss D: 186.80, loss G: -2.85\n",
      "Epoch [307/500] Batch 31/34                   Loss D: 958.06, loss G: -2.85\n",
      "Epoch [308/500] Batch 16/34                   Loss D: 213.60, loss G: -2.85\n",
      "Epoch [308/500] Batch 31/34                   Loss D: 10.03, loss G: -2.85\n",
      "Epoch [309/500] Batch 16/34                   Loss D: 9.99, loss G: -2.85\n",
      "Epoch [309/500] Batch 31/34                   Loss D: 11.79, loss G: -2.85\n",
      "Epoch [310/500] Batch 16/34                   Loss D: 17.60, loss G: -2.85\n",
      "Epoch [310/500] Batch 31/34                   Loss D: 12.31, loss G: -2.85\n",
      "Epoch [311/500] Batch 16/34                   Loss D: 9.01, loss G: -2.92\n",
      "Epoch [311/500] Batch 31/34                   Loss D: 9.94, loss G: -3.05\n",
      "Epoch [312/500] Batch 16/34                   Loss D: 570.60, loss G: -3.28\n",
      "Epoch [312/500] Batch 31/34                   Loss D: 9.93, loss G: -3.28\n",
      "Epoch [313/500] Batch 16/34                   Loss D: 10.54, loss G: -3.28\n",
      "Epoch [313/500] Batch 31/34                   Loss D: 11.02, loss G: -3.28\n",
      "Epoch [314/500] Batch 16/34                   Loss D: 16.23, loss G: -3.28\n",
      "Epoch [314/500] Batch 31/34                   Loss D: 16.87, loss G: -3.28\n",
      "Epoch [315/500] Batch 16/34                   Loss D: 56.54, loss G: -3.28\n",
      "Epoch [315/500] Batch 31/34                   Loss D: 9.41, loss G: -3.28\n",
      "Epoch [316/500] Batch 16/34                   Loss D: 36.26, loss G: -3.67\n",
      "Epoch [316/500] Batch 31/34                   Loss D: 9.97, loss G: -3.66\n",
      "Epoch [317/500] Batch 16/34                   Loss D: 11.10, loss G: -3.42\n",
      "Epoch [317/500] Batch 31/34                   Loss D: 9.99, loss G: -3.42\n",
      "Epoch [318/500] Batch 16/34                   Loss D: 9.96, loss G: -3.42\n",
      "Epoch [318/500] Batch 31/34                   Loss D: 10.64, loss G: -3.42\n",
      "Epoch [319/500] Batch 16/34                   Loss D: 171.55, loss G: -3.42\n",
      "Epoch [319/500] Batch 31/34                   Loss D: 10.03, loss G: -3.42\n",
      "Epoch [320/500] Batch 16/34                   Loss D: 10.90, loss G: -3.42\n",
      "Epoch [320/500] Batch 31/34                   Loss D: 10.06, loss G: -3.42\n",
      "Epoch [321/500] Batch 16/34                   Loss D: 221.13, loss G: -3.85\n",
      "Epoch [321/500] Batch 31/34                   Loss D: 20.27, loss G: -3.83\n",
      "Epoch [322/500] Batch 16/34                   Loss D: 10.09, loss G: -3.87\n",
      "Epoch [322/500] Batch 31/34                   Loss D: 18.54, loss G: -3.87\n",
      "Epoch [323/500] Batch 16/34                   Loss D: 10.43, loss G: -3.87\n",
      "Epoch [323/500] Batch 31/34                   Loss D: 10.72, loss G: -3.87\n",
      "Epoch [324/500] Batch 16/34                   Loss D: 156.06, loss G: -3.87\n",
      "Epoch [324/500] Batch 31/34                   Loss D: 11.78, loss G: -3.87\n",
      "Epoch [325/500] Batch 16/34                   Loss D: 571.33, loss G: -3.87\n",
      "Epoch [325/500] Batch 31/34                   Loss D: 5222.11, loss G: -3.87\n",
      "Epoch [326/500] Batch 16/34                   Loss D: 5965.14, loss G: -4.04\n",
      "Epoch [326/500] Batch 31/34                   Loss D: 25.38, loss G: -4.27\n",
      "Epoch [327/500] Batch 16/34                   Loss D: 1741.30, loss G: -4.34\n",
      "Epoch [327/500] Batch 31/34                   Loss D: 9.95, loss G: -4.34\n",
      "Epoch [328/500] Batch 16/34                   Loss D: 33.09, loss G: -4.34\n",
      "Epoch [328/500] Batch 31/34                   Loss D: 179.08, loss G: -4.34\n",
      "Epoch [329/500] Batch 16/34                   Loss D: 25.37, loss G: -4.34\n",
      "Epoch [329/500] Batch 31/34                   Loss D: 9.94, loss G: -4.34\n",
      "Epoch [330/500] Batch 16/34                   Loss D: 9.78, loss G: -4.34\n",
      "Epoch [330/500] Batch 31/34                   Loss D: 7929.14, loss G: -4.34\n",
      "Epoch [331/500] Batch 16/34                   Loss D: 9.97, loss G: -3.67\n",
      "Epoch [331/500] Batch 31/34                   Loss D: 170.53, loss G: -3.52\n",
      "Epoch [332/500] Batch 16/34                   Loss D: 10.76, loss G: -3.55\n",
      "Epoch [332/500] Batch 31/34                   Loss D: 64.54, loss G: -3.55\n",
      "Epoch [333/500] Batch 16/34                   Loss D: 10.17, loss G: -3.55\n",
      "Epoch [333/500] Batch 31/34                   Loss D: 12.04, loss G: -3.55\n",
      "Epoch [334/500] Batch 16/34                   Loss D: 9.23, loss G: -3.55\n",
      "Epoch [334/500] Batch 31/34                   Loss D: 9.05, loss G: -3.55\n",
      "Epoch [335/500] Batch 16/34                   Loss D: 9.94, loss G: -3.55\n",
      "Epoch [335/500] Batch 31/34                   Loss D: 9.94, loss G: -3.55\n",
      "Epoch [336/500] Batch 16/34                   Loss D: 11.03, loss G: -2.89\n",
      "Epoch [336/500] Batch 31/34                   Loss D: 172.73, loss G: -2.70\n",
      "Epoch [337/500] Batch 16/34                   Loss D: 52.69, loss G: -3.17\n",
      "Epoch [337/500] Batch 31/34                   Loss D: 12.46, loss G: -3.17\n",
      "Epoch [338/500] Batch 16/34                   Loss D: 20.76, loss G: -3.17\n",
      "Epoch [338/500] Batch 31/34                   Loss D: 12.43, loss G: -3.17\n",
      "Epoch [339/500] Batch 16/34                   Loss D: 325.89, loss G: -3.17\n",
      "Epoch [339/500] Batch 31/34                   Loss D: 508.73, loss G: -3.17\n",
      "Epoch [340/500] Batch 16/34                   Loss D: 10.28, loss G: -3.17\n",
      "Epoch [340/500] Batch 31/34                   Loss D: 248.18, loss G: -3.17\n",
      "Epoch [341/500] Batch 16/34                   Loss D: 4549.09, loss G: -2.48\n",
      "Epoch [341/500] Batch 31/34                   Loss D: 9.85, loss G: -2.54\n",
      "Epoch [342/500] Batch 16/34                   Loss D: 30.52, loss G: -3.61\n",
      "Epoch [342/500] Batch 31/34                   Loss D: 133.71, loss G: -3.61\n",
      "Epoch [343/500] Batch 16/34                   Loss D: 18.91, loss G: -3.61\n",
      "Epoch [343/500] Batch 31/34                   Loss D: 9.89, loss G: -3.61\n",
      "Epoch [344/500] Batch 16/34                   Loss D: 9.86, loss G: -3.61\n",
      "Epoch [344/500] Batch 31/34                   Loss D: 10.00, loss G: -3.61\n",
      "Epoch [345/500] Batch 16/34                   Loss D: 19.30, loss G: -3.61\n",
      "Epoch [345/500] Batch 31/34                   Loss D: 9.93, loss G: -3.61\n",
      "Epoch [346/500] Batch 16/34                   Loss D: 9.94, loss G: -2.38\n",
      "Epoch [346/500] Batch 31/34                   Loss D: 9.97, loss G: -2.33\n",
      "Epoch [347/500] Batch 16/34                   Loss D: 10.85, loss G: -3.72\n",
      "Epoch [347/500] Batch 31/34                   Loss D: 9.73, loss G: -3.72\n",
      "Epoch [348/500] Batch 16/34                   Loss D: 9.99, loss G: -3.72\n",
      "Epoch [348/500] Batch 31/34                   Loss D: 9.68, loss G: -3.72\n",
      "Epoch [349/500] Batch 16/34                   Loss D: 10.00, loss G: -3.72\n",
      "Epoch [349/500] Batch 31/34                   Loss D: 10.00, loss G: -3.72\n",
      "Epoch [350/500] Batch 16/34                   Loss D: 10.00, loss G: -3.72\n",
      "Epoch [350/500] Batch 31/34                   Loss D: 10.00, loss G: -3.72\n",
      "Epoch [351/500] Batch 16/34                   Loss D: 10.00, loss G: -2.14\n",
      "Epoch [351/500] Batch 31/34                   Loss D: 9.96, loss G: -2.17\n",
      "Epoch [352/500] Batch 16/34                   Loss D: 9.98, loss G: -2.65\n",
      "Epoch [352/500] Batch 31/34                   Loss D: 10.00, loss G: -2.65\n",
      "Epoch [353/500] Batch 16/34                   Loss D: 10.00, loss G: -2.65\n",
      "Epoch [353/500] Batch 31/34                   Loss D: 10.00, loss G: -2.65\n",
      "Epoch [354/500] Batch 16/34                   Loss D: 10.00, loss G: -2.65\n",
      "Epoch [354/500] Batch 31/34                   Loss D: 9.99, loss G: -2.65\n",
      "Epoch [355/500] Batch 16/34                   Loss D: 10.00, loss G: -2.65\n",
      "Epoch [355/500] Batch 31/34                   Loss D: 9.90, loss G: -2.65\n",
      "Epoch [356/500] Batch 16/34                   Loss D: 9.98, loss G: -2.80\n",
      "Epoch [356/500] Batch 31/34                   Loss D: 8.99, loss G: -2.96\n",
      "Epoch [357/500] Batch 16/34                   Loss D: 23.34, loss G: -2.36\n",
      "Epoch [357/500] Batch 31/34                   Loss D: 9.90, loss G: -2.36\n",
      "Epoch [358/500] Batch 16/34                   Loss D: 9.86, loss G: -2.36\n",
      "Epoch [358/500] Batch 31/34                   Loss D: 10.00, loss G: -2.36\n",
      "Epoch [359/500] Batch 16/34                   Loss D: 27.06, loss G: -2.36\n",
      "Epoch [359/500] Batch 31/34                   Loss D: 10.00, loss G: -2.36\n",
      "Epoch [360/500] Batch 16/34                   Loss D: 10.00, loss G: -2.36\n",
      "Epoch [360/500] Batch 31/34                   Loss D: 8.64, loss G: -2.36\n",
      "Epoch [361/500] Batch 16/34                   Loss D: 10.00, loss G: -1.87\n",
      "Epoch [361/500] Batch 31/34                   Loss D: 10.00, loss G: -2.07\n",
      "Epoch [362/500] Batch 16/34                   Loss D: 10.00, loss G: -2.21\n",
      "Epoch [362/500] Batch 31/34                   Loss D: 14.93, loss G: -2.21\n",
      "Epoch [363/500] Batch 16/34                   Loss D: 10.00, loss G: -2.21\n",
      "Epoch [363/500] Batch 31/34                   Loss D: 10.00, loss G: -2.21\n",
      "Epoch [364/500] Batch 16/34                   Loss D: 9.99, loss G: -2.21\n",
      "Epoch [364/500] Batch 31/34                   Loss D: 10.00, loss G: -2.21\n",
      "Epoch [365/500] Batch 16/34                   Loss D: 11.42, loss G: -2.21\n",
      "Epoch [365/500] Batch 31/34                   Loss D: 12.79, loss G: -2.21\n",
      "Epoch [366/500] Batch 16/34                   Loss D: 9.60, loss G: -1.18\n",
      "Epoch [366/500] Batch 31/34                   Loss D: 10.00, loss G: -1.11\n",
      "Epoch [367/500] Batch 16/34                   Loss D: 101.42, loss G: -1.85\n",
      "Epoch [367/500] Batch 31/34                   Loss D: 26.88, loss G: -1.85\n",
      "Epoch [368/500] Batch 16/34                   Loss D: 10.00, loss G: -1.85\n",
      "Epoch [368/500] Batch 31/34                   Loss D: 9.98, loss G: -1.85\n",
      "Epoch [369/500] Batch 16/34                   Loss D: 8.77, loss G: -1.85\n",
      "Epoch [369/500] Batch 31/34                   Loss D: 10.00, loss G: -1.85\n",
      "Epoch [370/500] Batch 16/34                   Loss D: 9.99, loss G: -1.85\n",
      "Epoch [370/500] Batch 31/34                   Loss D: 8.80, loss G: -1.85\n",
      "Epoch [371/500] Batch 16/34                   Loss D: 10.00, loss G: -2.09\n",
      "Epoch [371/500] Batch 31/34                   Loss D: 10.00, loss G: -2.20\n",
      "Epoch [372/500] Batch 16/34                   Loss D: 10.44, loss G: -1.97\n",
      "Epoch [372/500] Batch 31/34                   Loss D: 10.00, loss G: -1.97\n",
      "Epoch [373/500] Batch 16/34                   Loss D: 9.99, loss G: -1.97\n",
      "Epoch [373/500] Batch 31/34                   Loss D: 15.31, loss G: -1.97\n",
      "Epoch [374/500] Batch 16/34                   Loss D: 10.00, loss G: -1.97\n",
      "Epoch [374/500] Batch 31/34                   Loss D: 10.00, loss G: -1.97\n",
      "Epoch [375/500] Batch 16/34                   Loss D: 10.00, loss G: -1.97\n",
      "Epoch [375/500] Batch 31/34                   Loss D: 10.00, loss G: -1.97\n",
      "Epoch [376/500] Batch 16/34                   Loss D: 10.00, loss G: -2.19\n",
      "Epoch [376/500] Batch 31/34                   Loss D: 8.49, loss G: -1.84\n",
      "Epoch [377/500] Batch 16/34                   Loss D: 10.00, loss G: -1.16\n",
      "Epoch [377/500] Batch 31/34                   Loss D: 9.99, loss G: -1.16\n",
      "Epoch [378/500] Batch 16/34                   Loss D: 10.00, loss G: -1.16\n",
      "Epoch [378/500] Batch 31/34                   Loss D: 10.00, loss G: -1.16\n",
      "Epoch [379/500] Batch 16/34                   Loss D: 9.20, loss G: -1.16\n",
      "Epoch [379/500] Batch 31/34                   Loss D: 9.98, loss G: -1.16\n",
      "Epoch [380/500] Batch 16/34                   Loss D: 10.00, loss G: -1.16\n",
      "Epoch [380/500] Batch 31/34                   Loss D: 8.94, loss G: -1.16\n",
      "Epoch [381/500] Batch 16/34                   Loss D: 10.00, loss G: -2.08\n",
      "Epoch [381/500] Batch 31/34                   Loss D: 10.00, loss G: -1.79\n",
      "Epoch [382/500] Batch 16/34                   Loss D: 10.00, loss G: -2.17\n",
      "Epoch [382/500] Batch 31/34                   Loss D: 9.96, loss G: -2.17\n",
      "Epoch [383/500] Batch 16/34                   Loss D: 10.00, loss G: -2.17\n",
      "Epoch [383/500] Batch 31/34                   Loss D: 10.00, loss G: -2.17\n",
      "Epoch [384/500] Batch 16/34                   Loss D: 9.68, loss G: -2.17\n",
      "Epoch [384/500] Batch 31/34                   Loss D: 9.99, loss G: -2.17\n",
      "Epoch [385/500] Batch 16/34                   Loss D: 10.00, loss G: -2.17\n",
      "Epoch [385/500] Batch 31/34                   Loss D: 9.99, loss G: -2.17\n",
      "Epoch [386/500] Batch 16/34                   Loss D: 10.00, loss G: -1.77\n",
      "Epoch [386/500] Batch 31/34                   Loss D: 10.00, loss G: -2.03\n",
      "Epoch [387/500] Batch 16/34                   Loss D: 10.54, loss G: -3.58\n",
      "Epoch [387/500] Batch 31/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [388/500] Batch 16/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [388/500] Batch 31/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [389/500] Batch 16/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [389/500] Batch 31/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [390/500] Batch 16/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [390/500] Batch 31/34                   Loss D: 10.00, loss G: -3.58\n",
      "Epoch [391/500] Batch 16/34                   Loss D: 7.55, loss G: -2.58\n",
      "Epoch [391/500] Batch 31/34                   Loss D: 9.99, loss G: -3.85\n",
      "Epoch [392/500] Batch 16/34                   Loss D: 10.00, loss G: -3.42\n",
      "Epoch [392/500] Batch 31/34                   Loss D: 10.00, loss G: -3.42\n",
      "Epoch [393/500] Batch 16/34                   Loss D: 10.00, loss G: -3.42\n",
      "Epoch [393/500] Batch 31/34                   Loss D: 9.50, loss G: -3.42\n",
      "Epoch [394/500] Batch 16/34                   Loss D: 9.62, loss G: -3.42\n",
      "Epoch [394/500] Batch 31/34                   Loss D: 10.00, loss G: -3.42\n",
      "Epoch [395/500] Batch 16/34                   Loss D: 9.75, loss G: -3.42\n",
      "Epoch [395/500] Batch 31/34                   Loss D: 10.00, loss G: -3.42\n",
      "Epoch [396/500] Batch 16/34                   Loss D: 10.00, loss G: -2.89\n",
      "Epoch [396/500] Batch 31/34                   Loss D: 11.20, loss G: -2.52\n",
      "Epoch [397/500] Batch 16/34                   Loss D: 10.00, loss G: -3.19\n",
      "Epoch [397/500] Batch 31/34                   Loss D: 9.97, loss G: -3.19\n",
      "Epoch [398/500] Batch 16/34                   Loss D: 9.99, loss G: -3.19\n",
      "Epoch [398/500] Batch 31/34                   Loss D: 10.00, loss G: -3.19\n",
      "Epoch [399/500] Batch 16/34                   Loss D: 10.00, loss G: -3.19\n",
      "Epoch [399/500] Batch 31/34                   Loss D: 10.00, loss G: -3.19\n",
      "Epoch [400/500] Batch 16/34                   Loss D: 10.00, loss G: -3.19\n",
      "Epoch [400/500] Batch 31/34                   Loss D: 9.98, loss G: -3.19\n",
      "Epoch [401/500] Batch 16/34                   Loss D: 13.40, loss G: -2.58\n",
      "Epoch [401/500] Batch 31/34                   Loss D: 10.00, loss G: -2.94\n",
      "Epoch [402/500] Batch 16/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [402/500] Batch 31/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [403/500] Batch 16/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [403/500] Batch 31/34                   Loss D: 9.78, loss G: -2.69\n",
      "Epoch [404/500] Batch 16/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [404/500] Batch 31/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [405/500] Batch 16/34                   Loss D: 9.95, loss G: -2.69\n",
      "Epoch [405/500] Batch 31/34                   Loss D: 10.00, loss G: -2.69\n",
      "Epoch [406/500] Batch 16/34                   Loss D: 10.00, loss G: -1.52\n",
      "Epoch [406/500] Batch 31/34                   Loss D: 9.98, loss G: -1.88\n",
      "Epoch [407/500] Batch 16/34                   Loss D: 10.00, loss G: -0.66\n",
      "Epoch [407/500] Batch 31/34                   Loss D: 10.00, loss G: -0.66\n",
      "Epoch [408/500] Batch 16/34                   Loss D: 9.98, loss G: -0.66\n",
      "Epoch [408/500] Batch 31/34                   Loss D: 9.99, loss G: -0.66\n",
      "Epoch [409/500] Batch 16/34                   Loss D: 9.99, loss G: -0.66\n",
      "Epoch [409/500] Batch 31/34                   Loss D: 10.00, loss G: -0.66\n",
      "Epoch [410/500] Batch 16/34                   Loss D: 9.98, loss G: -0.66\n",
      "Epoch [410/500] Batch 31/34                   Loss D: 9.98, loss G: -0.66\n",
      "Epoch [411/500] Batch 16/34                   Loss D: 10.00, loss G: -6.24\n",
      "Epoch [411/500] Batch 31/34                   Loss D: 10.00, loss G: -5.62\n",
      "Epoch [412/500] Batch 16/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [412/500] Batch 31/34                   Loss D: 9.92, loss G: -3.64\n",
      "Epoch [413/500] Batch 16/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [413/500] Batch 31/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [414/500] Batch 16/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [414/500] Batch 31/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [415/500] Batch 16/34                   Loss D: 8.09, loss G: -3.64\n",
      "Epoch [415/500] Batch 31/34                   Loss D: 10.00, loss G: -3.64\n",
      "Epoch [416/500] Batch 16/34                   Loss D: 8.27, loss G: -5.26\n",
      "Epoch [416/500] Batch 31/34                   Loss D: 10.00, loss G: -4.78\n",
      "Epoch [417/500] Batch 16/34                   Loss D: 7.63, loss G: -3.01\n",
      "Epoch [417/500] Batch 31/34                   Loss D: 9.99, loss G: -3.01\n",
      "Epoch [418/500] Batch 16/34                   Loss D: 10.00, loss G: -3.01\n",
      "Epoch [418/500] Batch 31/34                   Loss D: 9.90, loss G: -3.01\n",
      "Epoch [419/500] Batch 16/34                   Loss D: 8.47, loss G: -3.01\n",
      "Epoch [419/500] Batch 31/34                   Loss D: 9.22, loss G: -3.01\n",
      "Epoch [420/500] Batch 16/34                   Loss D: 8.83, loss G: -3.01\n",
      "Epoch [420/500] Batch 31/34                   Loss D: 9.89, loss G: -3.01\n",
      "Epoch [421/500] Batch 16/34                   Loss D: 10.02, loss G: -8.43\n",
      "Epoch [421/500] Batch 31/34                   Loss D: 808.75, loss G: 0.44\n",
      "Epoch [422/500] Batch 16/34                   Loss D: 597.06, loss G: -3.44\n",
      "Epoch [422/500] Batch 31/34                   Loss D: 2314.93, loss G: -3.44\n",
      "Epoch [423/500] Batch 16/34                   Loss D: 219.99, loss G: -3.44\n",
      "Epoch [423/500] Batch 31/34                   Loss D: 57.39, loss G: -3.44\n",
      "Epoch [424/500] Batch 16/34                   Loss D: 14.55, loss G: -3.44\n",
      "Epoch [424/500] Batch 31/34                   Loss D: 137.43, loss G: -3.44\n",
      "Epoch [425/500] Batch 16/34                   Loss D: 277.32, loss G: -3.44\n",
      "Epoch [425/500] Batch 31/34                   Loss D: 112.88, loss G: -3.44\n",
      "Epoch [426/500] Batch 16/34                   Loss D: 179.04, loss G: 0.24\n",
      "Epoch [426/500] Batch 31/34                   Loss D: 39.06, loss G: 0.11\n",
      "Epoch [427/500] Batch 16/34                   Loss D: 19.28, loss G: -2.26\n",
      "Epoch [427/500] Batch 31/34                   Loss D: 61.40, loss G: -2.26\n",
      "Epoch [428/500] Batch 16/34                   Loss D: 9.94, loss G: -2.26\n",
      "Epoch [428/500] Batch 31/34                   Loss D: 193.08, loss G: -2.26\n",
      "Epoch [429/500] Batch 16/34                   Loss D: 20.16, loss G: -2.26\n",
      "Epoch [429/500] Batch 31/34                   Loss D: 107.51, loss G: -2.26\n",
      "Epoch [430/500] Batch 16/34                   Loss D: 11.28, loss G: -2.26\n",
      "Epoch [430/500] Batch 31/34                   Loss D: 10.66, loss G: -2.26\n",
      "Epoch [431/500] Batch 16/34                   Loss D: 9.99, loss G: -0.05\n",
      "Epoch [431/500] Batch 31/34                   Loss D: 9.96, loss G: -0.54\n",
      "Epoch [432/500] Batch 16/34                   Loss D: 10.46, loss G: -0.08\n",
      "Epoch [432/500] Batch 31/34                   Loss D: 9.99, loss G: -0.08\n",
      "Epoch [433/500] Batch 16/34                   Loss D: 39.34, loss G: -0.08\n",
      "Epoch [433/500] Batch 31/34                   Loss D: 9.54, loss G: -0.08\n",
      "Epoch [434/500] Batch 16/34                   Loss D: 10.00, loss G: -0.08\n",
      "Epoch [434/500] Batch 31/34                   Loss D: 9.77, loss G: -0.08\n",
      "Epoch [435/500] Batch 16/34                   Loss D: 10.00, loss G: -0.08\n",
      "Epoch [435/500] Batch 31/34                   Loss D: 10.00, loss G: -0.08\n",
      "Epoch [436/500] Batch 16/34                   Loss D: 10.42, loss G: -0.72\n",
      "Epoch [436/500] Batch 31/34                   Loss D: 10.00, loss G: -0.30\n",
      "Epoch [437/500] Batch 16/34                   Loss D: 11.28, loss G: -1.31\n",
      "Epoch [437/500] Batch 31/34                   Loss D: 2823.86, loss G: -1.31\n",
      "Epoch [438/500] Batch 16/34                   Loss D: 9.98, loss G: -1.31\n",
      "Epoch [438/500] Batch 31/34                   Loss D: 9.95, loss G: -1.31\n",
      "Epoch [439/500] Batch 16/34                   Loss D: 44.09, loss G: -1.31\n",
      "Epoch [439/500] Batch 31/34                   Loss D: 10.00, loss G: -1.31\n",
      "Epoch [440/500] Batch 16/34                   Loss D: 10.53, loss G: -1.31\n",
      "Epoch [440/500] Batch 31/34                   Loss D: 10.04, loss G: -1.31\n",
      "Epoch [441/500] Batch 16/34                   Loss D: 10.00, loss G: -1.95\n",
      "Epoch [441/500] Batch 31/34                   Loss D: 10.00, loss G: -1.99\n",
      "Epoch [442/500] Batch 16/34                   Loss D: 11.36, loss G: -0.11\n",
      "Epoch [442/500] Batch 31/34                   Loss D: 19.96, loss G: -0.11\n",
      "Epoch [443/500] Batch 16/34                   Loss D: 9.55, loss G: -0.11\n",
      "Epoch [443/500] Batch 31/34                   Loss D: 8.82, loss G: -0.11\n",
      "Epoch [444/500] Batch 16/34                   Loss D: 9.71, loss G: -0.11\n",
      "Epoch [444/500] Batch 31/34                   Loss D: 9.09, loss G: -0.11\n",
      "Epoch [445/500] Batch 16/34                   Loss D: 9.92, loss G: -0.11\n",
      "Epoch [445/500] Batch 31/34                   Loss D: 10.00, loss G: -0.11\n",
      "Epoch [446/500] Batch 16/34                   Loss D: 10.00, loss G: -6.33\n",
      "Epoch [446/500] Batch 31/34                   Loss D: 10.00, loss G: -6.46\n",
      "Epoch [447/500] Batch 16/34                   Loss D: 9.96, loss G: -3.85\n",
      "Epoch [447/500] Batch 31/34                   Loss D: 9.97, loss G: -3.85\n",
      "Epoch [448/500] Batch 16/34                   Loss D: 36.67, loss G: -3.85\n",
      "Epoch [448/500] Batch 31/34                   Loss D: 9.98, loss G: -3.85\n",
      "Epoch [449/500] Batch 16/34                   Loss D: 11.53, loss G: -3.85\n",
      "Epoch [449/500] Batch 31/34                   Loss D: 10.00, loss G: -3.85\n",
      "Epoch [450/500] Batch 16/34                   Loss D: 11.00, loss G: -3.85\n",
      "Epoch [450/500] Batch 31/34                   Loss D: 9.66, loss G: -3.85\n",
      "Epoch [451/500] Batch 16/34                   Loss D: 11.85, loss G: -5.41\n",
      "Epoch [451/500] Batch 31/34                   Loss D: 9.97, loss G: -4.03\n",
      "Epoch [452/500] Batch 16/34                   Loss D: 10.00, loss G: -1.96\n",
      "Epoch [452/500] Batch 31/34                   Loss D: 9.26, loss G: -1.96\n",
      "Epoch [453/500] Batch 16/34                   Loss D: 9.77, loss G: -1.96\n",
      "Epoch [453/500] Batch 31/34                   Loss D: 10.00, loss G: -1.96\n",
      "Epoch [454/500] Batch 16/34                   Loss D: 9.88, loss G: -1.96\n",
      "Epoch [454/500] Batch 31/34                   Loss D: 9.96, loss G: -1.96\n",
      "Epoch [455/500] Batch 16/34                   Loss D: 9.33, loss G: -1.96\n",
      "Epoch [455/500] Batch 31/34                   Loss D: 10.00, loss G: -1.96\n",
      "Epoch [456/500] Batch 16/34                   Loss D: 9.99, loss G: -5.24\n",
      "Epoch [456/500] Batch 31/34                   Loss D: 10.67, loss G: -5.15\n",
      "Epoch [457/500] Batch 16/34                   Loss D: 9.50, loss G: -3.37\n",
      "Epoch [457/500] Batch 31/34                   Loss D: 9.72, loss G: -3.37\n",
      "Epoch [458/500] Batch 16/34                   Loss D: 20.42, loss G: -3.37\n",
      "Epoch [458/500] Batch 31/34                   Loss D: 9.99, loss G: -3.37\n",
      "Epoch [459/500] Batch 16/34                   Loss D: 9.32, loss G: -3.37\n",
      "Epoch [459/500] Batch 31/34                   Loss D: 10.00, loss G: -3.37\n",
      "Epoch [460/500] Batch 16/34                   Loss D: 15.96, loss G: -3.37\n",
      "Epoch [460/500] Batch 31/34                   Loss D: 10.00, loss G: -3.37\n",
      "Epoch [461/500] Batch 16/34                   Loss D: 10.00, loss G: 1.76\n",
      "Epoch [461/500] Batch 31/34                   Loss D: 10.30, loss G: 2.12\n",
      "Epoch [462/500] Batch 16/34                   Loss D: 10.00, loss G: 3.64\n",
      "Epoch [462/500] Batch 31/34                   Loss D: 9.76, loss G: 3.64\n",
      "Epoch [463/500] Batch 16/34                   Loss D: 10.00, loss G: 3.64\n",
      "Epoch [463/500] Batch 31/34                   Loss D: 10.14, loss G: 3.64\n",
      "Epoch [464/500] Batch 16/34                   Loss D: 9.98, loss G: 3.64\n",
      "Epoch [464/500] Batch 31/34                   Loss D: 9.94, loss G: 3.64\n",
      "Epoch [465/500] Batch 16/34                   Loss D: 9.50, loss G: 3.64\n",
      "Epoch [465/500] Batch 31/34                   Loss D: 11.91, loss G: 3.64\n",
      "Epoch [466/500] Batch 16/34                   Loss D: 9.98, loss G: 4.33\n",
      "Epoch [466/500] Batch 31/34                   Loss D: 217.31, loss G: 4.11\n",
      "Epoch [467/500] Batch 16/34                   Loss D: 10.00, loss G: 5.57\n",
      "Epoch [467/500] Batch 31/34                   Loss D: 10.00, loss G: 5.57\n",
      "Epoch [468/500] Batch 16/34                   Loss D: 8.47, loss G: 5.57\n",
      "Epoch [468/500] Batch 31/34                   Loss D: 10.00, loss G: 5.57\n",
      "Epoch [469/500] Batch 16/34                   Loss D: 9.42, loss G: 5.57\n",
      "Epoch [469/500] Batch 31/34                   Loss D: 9.88, loss G: 5.57\n",
      "Epoch [470/500] Batch 16/34                   Loss D: 9.83, loss G: 5.57\n",
      "Epoch [470/500] Batch 31/34                   Loss D: 10.00, loss G: 5.57\n",
      "Epoch [471/500] Batch 16/34                   Loss D: 9.95, loss G: -8.34\n",
      "Epoch [471/500] Batch 31/34                   Loss D: 9.90, loss G: -8.27\n",
      "Epoch [472/500] Batch 16/34                   Loss D: 9.48, loss G: -7.04\n",
      "Epoch [472/500] Batch 31/34                   Loss D: 10.60, loss G: -7.04\n",
      "Epoch [473/500] Batch 16/34                   Loss D: 10.00, loss G: -7.04\n",
      "Epoch [473/500] Batch 31/34                   Loss D: 9.22, loss G: -7.04\n",
      "Epoch [474/500] Batch 16/34                   Loss D: 10.00, loss G: -7.04\n",
      "Epoch [474/500] Batch 31/34                   Loss D: 10.00, loss G: -7.04\n",
      "Epoch [475/500] Batch 16/34                   Loss D: 10.00, loss G: -7.04\n",
      "Epoch [475/500] Batch 31/34                   Loss D: 10.00, loss G: -7.04\n",
      "Epoch [476/500] Batch 16/34                   Loss D: 10.00, loss G: -5.12\n",
      "Epoch [476/500] Batch 31/34                   Loss D: 10.00, loss G: -4.72\n",
      "Epoch [477/500] Batch 16/34                   Loss D: 9.35, loss G: -4.13\n",
      "Epoch [477/500] Batch 31/34                   Loss D: 8.88, loss G: -4.13\n",
      "Epoch [478/500] Batch 16/34                   Loss D: 9.16, loss G: -4.13\n",
      "Epoch [478/500] Batch 31/34                   Loss D: 9.24, loss G: -4.13\n",
      "Epoch [479/500] Batch 16/34                   Loss D: 9.58, loss G: -4.13\n",
      "Epoch [479/500] Batch 31/34                   Loss D: 8.28, loss G: -4.13\n",
      "Epoch [480/500] Batch 16/34                   Loss D: 9.58, loss G: -4.13\n",
      "Epoch [480/500] Batch 31/34                   Loss D: 10.00, loss G: -4.13\n",
      "Epoch [481/500] Batch 16/34                   Loss D: 10.00, loss G: -1.44\n",
      "Epoch [481/500] Batch 31/34                   Loss D: 10.00, loss G: -1.76\n",
      "Epoch [482/500] Batch 16/34                   Loss D: 9.99, loss G: -0.47\n",
      "Epoch [482/500] Batch 31/34                   Loss D: 9.99, loss G: -0.47\n",
      "Epoch [483/500] Batch 16/34                   Loss D: 29.78, loss G: -0.47\n",
      "Epoch [483/500] Batch 31/34                   Loss D: 10.00, loss G: -0.47\n",
      "Epoch [484/500] Batch 16/34                   Loss D: 8.42, loss G: -0.47\n",
      "Epoch [484/500] Batch 31/34                   Loss D: 9.96, loss G: -0.47\n",
      "Epoch [485/500] Batch 16/34                   Loss D: 9.95, loss G: -0.47\n",
      "Epoch [485/500] Batch 31/34                   Loss D: 9.75, loss G: -0.47\n",
      "Epoch [486/500] Batch 16/34                   Loss D: 10.00, loss G: 4.31\n",
      "Epoch [486/500] Batch 31/34                   Loss D: 18.01, loss G: 4.70\n",
      "Epoch [487/500] Batch 16/34                   Loss D: 10.00, loss G: 2.05\n",
      "Epoch [487/500] Batch 31/34                   Loss D: 10.00, loss G: 2.05\n",
      "Epoch [488/500] Batch 16/34                   Loss D: 9.30, loss G: 2.05\n",
      "Epoch [488/500] Batch 31/34                   Loss D: 12.67, loss G: 2.05\n",
      "Epoch [489/500] Batch 16/34                   Loss D: 9.87, loss G: 2.05\n",
      "Epoch [489/500] Batch 31/34                   Loss D: 7.69, loss G: 2.05\n",
      "Epoch [490/500] Batch 16/34                   Loss D: 9.97, loss G: 2.05\n",
      "Epoch [490/500] Batch 31/34                   Loss D: 19.56, loss G: 2.05\n",
      "Epoch [491/500] Batch 16/34                   Loss D: 9.70, loss G: 9.22\n",
      "Epoch [491/500] Batch 31/34                   Loss D: 10.21, loss G: 9.27\n",
      "Epoch [492/500] Batch 16/34                   Loss D: 9.95, loss G: 3.72\n",
      "Epoch [492/500] Batch 31/34                   Loss D: 10.00, loss G: 3.72\n",
      "Epoch [493/500] Batch 16/34                   Loss D: 10.00, loss G: 3.72\n",
      "Epoch [493/500] Batch 31/34                   Loss D: 31.60, loss G: 3.72\n",
      "Epoch [494/500] Batch 16/34                   Loss D: 8.31, loss G: 3.72\n",
      "Epoch [494/500] Batch 31/34                   Loss D: 9.97, loss G: 3.72\n",
      "Epoch [495/500] Batch 16/34                   Loss D: 8.64, loss G: 3.72\n",
      "Epoch [495/500] Batch 31/34                   Loss D: 10.00, loss G: 3.72\n",
      "Epoch [496/500] Batch 16/34                   Loss D: 10.42, loss G: 10.01\n",
      "Epoch [496/500] Batch 31/34                   Loss D: 9.99, loss G: 10.58\n",
      "Epoch [497/500] Batch 16/34                   Loss D: 10.00, loss G: 7.39\n",
      "Epoch [497/500] Batch 31/34                   Loss D: 8.94, loss G: 7.39\n",
      "Epoch [498/500] Batch 16/34                   Loss D: 9.31, loss G: 7.39\n",
      "Epoch [498/500] Batch 31/34                   Loss D: 9.94, loss G: 7.39\n",
      "Epoch [499/500] Batch 16/34                   Loss D: 10.00, loss G: 7.39\n",
      "Epoch [499/500] Batch 31/34                   Loss D: 9.96, loss G: 7.39\n",
      "Epoch [500/500] Batch 16/34                   Loss D: 87.69, loss G: 7.39\n",
      "Epoch [500/500] Batch 31/34                   Loss D: 15.04, loss G: 7.39\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Learning_rate = 1e-4\n",
    "Batch_size = 512\n",
    "seq_size = 4\n",
    "seq_len = 600\n",
    "noise_dim = 200\n",
    "num_epochs = 500\n",
    "features_cirtic = 300\n",
    "features_gen = 200\n",
    "Critic_iterations = 5\n",
    "Lamda_GP = 10\n",
    "\n",
    "#Lists\n",
    "gen_losses_W = []\n",
    "critic_losses_W = []\n",
    "# using dataloader to split data to batches\n",
    "dataloader = DataLoader(training_data, batch_size=Batch_size, shuffle=True)\n",
    "# calling generator and critic classes\n",
    "gen = Generator(noise_dim, features_gen,SelfAttentionLayer)\n",
    "critic = Critic(seq_len, features_cirtic,SelfAttentionLayer)\n",
    "# Initialize the weights: mean=0, std = 0.02\n",
    "Initialize_weights(gen)\n",
    "Initialize_weights(critic)\n",
    "# calling loss functions\n",
    "lossG = GANLossGenerator()\n",
    "lossD = GANLossDiscriminator()\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "# using adam optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=Learning_rate, betas=(0.0, 0.9))\n",
    "opt_critic= optim.Adam(critic.parameters(), lr=Learning_rate, betas=(0.0, 0.9))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for batch_idx, real in enumerate (dataloader):\n",
    "          # Train the critic\n",
    "      real = real.to(device)\n",
    "      # generate random noise\n",
    "      noise = torch.randn(len(real),noise_dim,1).to(device)\n",
    "      # passing the noise into generator to generate sequences\n",
    "      fake = gen(noise).to(device)\n",
    "      # Training the critic \n",
    "      critic_real = critic(real).reshape(-1)\n",
    "      critic_fake = critic(fake).reshape(-1)\n",
    "      # calling gradient penalty and calculate the loss\n",
    "      gp = GradientPenalty(critic, real,fake,device = device)\n",
    "      loss_critic = lossD.Wasserstein(critic_real, critic_fake)\n",
    "      loss_critic = loss_critic + Lamda_GP * gp\n",
    "      loss_Critic = torch.autograd.Variable(loss_critic, requires_grad = True)\n",
    "      # backpropagate and optimize critic model\n",
    "      critic.zero_grad()\n",
    "      loss_critic.backward(retain_graph = True)\n",
    "      opt_critic.step()\n",
    "      \n",
    "      # Training generator\n",
    "      # learining generator once for every 5 epochs\n",
    "      # calculate the loss\n",
    "      if epoch % Critic_iterations == 0:\n",
    "            loss_gen = lossG.Wasserstein(critic_fake)\n",
    "            loss_gen = torch.autograd.Variable(loss_gen, requires_grad = True)\n",
    "            # backpropagate and optimize the generator model\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()   \n",
    "      critic_losses_W.append(loss_critic.detach().float())\n",
    "      gen_losses_W.append(loss_gen.detach().float())\n",
    "      # print the losses occasionally\n",
    "      if batch_idx % 15 == 0 and batch_idx > 0:\n",
    "        print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx+1}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_critic:.2f}, loss G: {loss_gen:.2f}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(),\"WGAN_GP Generator.pt\")\n",
    "torch.save(critic.state_dict(),\"WGAN-GP Critic.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Visualize(gen_losses_W,critic_losses_W)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(GAN):\n",
    "    noise_dim = 200\n",
    "    gen_features = 200\n",
    "    seq_len = 600\n",
    "    disc_features =300\n",
    "    if GAN == 'DCGAN': \n",
    "        # Load Generator\n",
    "        gen = Generator(noise_dim, gen_features, SelfAttentionLayer)\n",
    "        gen.load_state_dict(torch.load('C:/Users/EGYPT_LAPTOP/Desktop/Saved models- Proteinea/DCGAN Generator.pt'))\n",
    "        # gen.eval()\n",
    "        # Load discriminator\n",
    "        disc = Discriminator(seq_len, disc_features, SelfAttentionLayer)\n",
    "        disc.load_state_dict(torch.load('C:/Users/EGYPT_LAPTOP/Desktop/Saved models- Proteinea/DCGAN Discriminator.pt'))\n",
    "        disc.eval()\n",
    "        return gen,disc\n",
    "    elif GAN == 'WGAN-GP':\n",
    "        # Load Generator\n",
    "        gen_wgan_gp = Generator(noise_dim, gen_features, SelfAttentionLayer)\n",
    "        gen_wgan_gp.load_state_dict(torch.load('C:/Users/EGYPT_LAPTOP/Desktop/Saved models- Proteinea/WGAN_GP Generator.pt'))\n",
    "        gen_wgan_gp.eval()\n",
    "        # Load Critic\n",
    "        critic_wgan_gp = Critic(seq_len, disc_features, SelfAttentionLayer)\n",
    "        critic_wgan_gp.load_state_dict(torch.load('C:/Users/EGYPT_LAPTOP/Desktop/Saved models- Proteinea/WGAN-GP Critic.pt'))\n",
    "        critic_wgan_gp.eval()\n",
    "        return gen,critic\n",
    "    else:\n",
    "        print(\"DCGAN and WGAN-GP only saved model we have\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate (no_of_sequence):\n",
    "    # create list and dictionary\n",
    "    fake_all = []\n",
    "    nucleotide = []\n",
    "    converter = {0: 'A', 1:'C',2:'G',3:'T'}\n",
    "    # Generate promoters\n",
    "    for i in range(no_of_sequence):\n",
    "        gen = Generator(200, 200,SelfAttentionLayer)\n",
    "        noise = torch.randn(1,200,1)\n",
    "        fake = gen(noise)\n",
    "        fake_all.append(fake)\n",
    "    Synthetic_promoters = torch.stack(fake_all)\n",
    "    # Convert generated promoters into nucleotides\n",
    "    for i in Synthetic_promoters:\n",
    "        sequence = []\n",
    "        for j in i[0]:\n",
    "            sequence.append(converter[int(j.argmax())])\n",
    "        nucleotide.append(''.join(sequence))\n",
    "    return nucleotide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "load('DCGAN')\n",
    "dcgan_promoters = Generate(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "load('WGAN-GP')\n",
    "wgan_promoters = Generate(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download file as FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_fasta(path, seqs, prefix):\n",
    "  fasta = \"\"\n",
    "  for i, seq in enumerate(seqs):\n",
    "    fasta += f\">{prefix}_{i}\\n{seq}\\n\"\n",
    "  print(fasta, file=open(path, 'a'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_fasta(\"dcgan-fasta.fasta\",dcgan_promoters,'dcgan')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_fasta(\"wgan-fasta.fasta\",wgan_promoters,'wgan-gp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maffit alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApplicationError",
     "evalue": "Non-zero return code 1 from '/usr/bin/mafft dcgan-fasta.fasta', message 'The system cannot find the path specified.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApplicationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\DEBI\\Uottawa\\Final project - Protienea\\Synthetic promoters via GAN\\Synthetic promoters via GAN.ipynb Cell 71\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dcgan_alignment \u001b[39m=\u001b[39m Maffit(\u001b[39m'\u001b[39;49m\u001b[39mdcgan-fasta.fasta\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdcgan_alignment\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dcgan_alignment\n",
      "\u001b[1;32md:\\DEBI\\Uottawa\\Final project - Protienea\\Synthetic promoters via GAN\\Synthetic promoters via GAN.ipynb Cell 71\u001b[0m in \u001b[0;36mMaffit\u001b[1;34m(in_file, out_file)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mafft_exe \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/usr/bin/mafft\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mafft_cline \u001b[39m=\u001b[39m MafftCommandline(mafft_exe, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39min_file)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m stdout, stderr \u001b[39m=\u001b[39m mafft_cline()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(out_file, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEBI/Uottawa/Final%20project%20-%20Protienea/Synthetic%20promoters%20via%20GAN/Synthetic%20promoters%20via%20GAN.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     handle\u001b[39m.\u001b[39mwrite(stdout)\n",
      "File \u001b[1;32mc:\\Users\\EGYPT_LAPTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\Bio\\Application\\__init__.py:574\u001b[0m, in \u001b[0;36mAbstractCommandline.__call__\u001b[1;34m(self, stdin, stdout, stderr, cwd, env)\u001b[0m\n\u001b[0;32m    571\u001b[0m     stderr_arg\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    573\u001b[0m \u001b[39mif\u001b[39;00m return_code:\n\u001b[1;32m--> 574\u001b[0m     \u001b[39mraise\u001b[39;00m ApplicationError(return_code, \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m), stdout_str, stderr_str)\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m stdout_str, stderr_str\n",
      "\u001b[1;31mApplicationError\u001b[0m: Non-zero return code 1 from '/usr/bin/mafft dcgan-fasta.fasta', message 'The system cannot find the path specified.'"
     ]
    }
   ],
   "source": [
    "dcgan_alignment = Maffit('dcgan-fasta.fasta','dcgan_alignment')\n",
    "dcgan_alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan_alignment = Maffit('D:/DEBI/Uottawa/Final project - Protienea/Data/drosophila melanogaster promoters.FASTA','wgan_alignment')\n",
    "wgan_alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actin 5C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Actin_5C = 'AAATCAATTCGCCAACACTTTCTTTTTTTCCCACGCCCTAAAACACCAGATCATCCATAAATGTACATACATACAGTATATGCATATTATAATCTGTAAAACTAGATCAGGTTCTTGAAAATAGTGACGTAGGCAGCCGTTTTGGCTGAAGCAGAAATTTTTGCCGGTTTTTCAAAGTTGTAGTTGCAAAAATGGAGAAAACCTTCGAGCATTCGTTCATATACACACACTCACGCGCAAAATAACGAGAGAGAGTGTATGTGTGTGTGAGAGAGCGAAAGCCAGACGACGGTTTGCTTTTCGCCTCGAAACATGACCATATATGGTCACAAAACTTGGCCGCCGCAATTCAACACACCAGCGCTCTCCTTCGCACCCATAGCGACCATGGCGCGGAGCGAGCGAGATGGCGAGAGCGAGCGACGCCTATGGCGACGTCGACGCAGGCAGCGATTGAAAAACGCAGTTAACTGGCATTCAACATTCACCAGCCACTTTCAGTCGGTTTATTCCAGTCATTCCTTTCAAACCGTGCGGTCGCTTAGCTCAGCCTCGCCACTTGCGTTTACAGTAGTTTTCACGCCTTGAATTTGTTAAATC'\n",
    "Actin_5C = Actin_5C.upper()\n",
    "len(Actin_5C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(promoters,seq2,out_file):\n",
    "    aligns = []\n",
    "    score = []\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    for i in range(len(promoters)):\n",
    "        alignments = pairwise2.align.globalxx(Actin_5C, promoters[i]) \n",
    "        for alignment in alignments: \n",
    "            aligns.append(format_alignment(*alignment))\n",
    "\n",
    "    with open(out_file, \"w\") as handle:\n",
    "        for i in aligns:\n",
    "            handle.write(i)\n",
    "\n",
    "    for i in range(len(promoters)):\n",
    "        alignments = aligner.align(Actin_5C, promoters[i])\n",
    "        score.append(alignments.score)\n",
    "    score = np.array(score)/len(promoters[0])\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pairwise(dcgan_promoters,Actin_5C,\"dcgan-actin.fasta\")\n",
    "np.where(score >= 0.85)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pairwise(wgan_promoters,Actin_5C,\"wgan-actin.fasta\")\n",
    "np.where(score >= 0.85)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL Drosophila melanogaster database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_database(Database_path):\n",
    "    \"\"\" read fasta file into two lists \n",
    "\n",
    "    Args:\n",
    "        Database_path (str): Path to fasta file\n",
    "    \"\"\"\n",
    "    Seqlist = []\n",
    "    IDlist = []\n",
    "    for record in SeqIO.parse(Database_path, \"fasta\"):\n",
    "        Seqlist.append(record.seq)\n",
    "        IDlist.append(record.id)\n",
    "        \n",
    "    return[IDlist, Seqlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligner(Wild, Target):\n",
    "    \"\"\"perfoem pairwise sequence alignemnt \n",
    "\n",
    "    Args:\n",
    "        Wild (str): Sequence of DNA\n",
    "        Target (str): Sequence of DNA\n",
    "\n",
    "    Returns:\n",
    "        _type_: Alignemnt data \n",
    "    \"\"\"\n",
    "    alignments = pairwise2.align.globalms(Wild, Target, 5, -4, -10000,-.1)\n",
    "    res = [alignments[0].seqA, alignments[0].seqB]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_calculator(Wild, Target):\n",
    "    \"\"\"Ckaculate alignment percentage \n",
    "\n",
    "    Args:\n",
    "        Wild (str): Sequence of DNA \n",
    "        Target (str): Sequence of DNA\n",
    "    \"\"\"\n",
    "    Seq_Identity = Levenshtein.ratio(Wild, Target) * 100\n",
    "    return(Seq_Identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read fasta files \n",
    "Refernce_database_ids, Refernce_database_seqs = read_fasta_database('C:/Users/EGYPT_LAPTOP/Desktop/New folder/Evaluation/drosophila_main_promoters.fasta')\n",
    "WGAN_database_ids, WGAN_database_seqs = read_fasta_database(\"C:/Users/EGYPT_LAPTOP/Desktop/New folder/wgan output/wgan-fasta.fasta\")\n",
    "DCGAN_database_ids, DCGAN_database_seqs = read_fasta_database(\"C:/Users/EGYPT_LAPTOP/Desktop/New folder/dcgan output/dcgan-fasta.fasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 #ref_seq_counter\n",
    "j = 0 #GANs_counter\n",
    "Ref_seq_ids = []\n",
    "DCGAN_seq_ids = []\n",
    "Similartity = []\n",
    "\n",
    "for DCGANseq in DCGAN_database_seqs:\n",
    "    for ref_seq in Refernce_database_seqs:\n",
    "        SeqA, SeqB = aligner(DCGANseq, ref_seq)\n",
    "        Identity = percentage_calculator(SeqA, SeqB)\n",
    "        Ref_seq_ids.append(Refernce_database_ids[i])\n",
    "        DCGAN_seq_ids.append(DCGAN_database_ids[j])\n",
    "        Similartity.append(Identity)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCGAN_Results = pd.DataFrame(\n",
    "    {'Ref_seq_ids': Ref_seq_ids,\n",
    "     'DCAN_seq_ids': DCGAN_seq_ids,\n",
    "     'Similartity': Similartity\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCGAN_Results.to_csv('C:/Users/EGYPT_LAPTOP/Desktop/New folder/dcgan output/DCGAN_Results.csv', index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 #ref_seq_counter\n",
    "j = 0 #GANs_counter\n",
    "Ref_seq_ids = []\n",
    "WGAN_seq_ids = []\n",
    "Similartity = []\n",
    "\n",
    "for WGANseq in WGAN_database_seqs:\n",
    "    for ref_seq in Refernce_database_seqs:\n",
    "        SeqA, SeqB = aligner(WGANseq, ref_seq)\n",
    "        Identity = percentage_calculator(SeqA, SeqB)\n",
    "        Ref_seq_ids.append(Refernce_database_ids[i])\n",
    "        WGAN_seq_ids.append(WGAN_database_ids[j])\n",
    "        Similartity.append(Identity)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WGAN_Results = pd.DataFrame(\n",
    "    {'Ref_seq_ids': Ref_seq_ids,\n",
    "     'DCAN_seq_ids': WGAN_seq_ids,\n",
    "     'Similartity': Similartity\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WGAN_Results.to_csv('C:/Users/EGYPT_LAPTOP/Desktop/New folder/wgan output/WGAN_Results.csv', index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read fasta files \n",
    "DCGAN_Results = pd.read_csv('C:/Users/EGYPT_LAPTOP/Desktop/New folder/Evaluation/DCGAN_Results.csv')\n",
    "WGAN_Results = pd.read_csv('C:/Users/EGYPT_LAPTOP/Desktop/New folder/Evaluation/WGAN_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCGAN = []\n",
    "WGAN = []\n",
    "Refs = []\n",
    "DCGAN_sorted = DCGAN_Results.sort_values('Similartity',ascending=False)\n",
    "WGAN_sorted = WGAN_Results.sort_values('Similartity',ascending=False)\n",
    "Ref1 = DCGAN_sorted.iloc[:20,0].unique()\n",
    "Ref2 = WGAN_sorted.iloc[:20,0].unique()\n",
    "Ref = np.concatenate([Ref1,Ref2])\n",
    "Ref = np.unique(Ref)\n",
    "for i in range(20):\n",
    "    for j in range(len(DCGAN_database_ids)):\n",
    "        if DCGAN_sorted.iloc[i,1] == DCGAN_database_ids[j]:\n",
    "            DCGAN.append(DCGAN_database_seqs[j])\n",
    "            continue\n",
    "    for j in range(len(WGAN_database_ids)):\n",
    "        if WGAN_sorted.iloc[i,1] == WGAN_database_ids[j]:\n",
    "            WGAN.append(WGAN_database_seqs[j])\n",
    "            continue\n",
    "\n",
    "for i in Ref:\n",
    "    for j in range(len(Refernce_database_ids)):\n",
    "        if i == Refernce_database_ids[j]:\n",
    "            Refs.append(Refernce_database_seqs[j])\n",
    "            continue\n",
    "Top_Similarity_seqs = Refs + DCGAN + WGAN\n",
    "Top_Similarity_ids = pd.DataFrame(np.concatenate([Ref,DCGAN_sorted.iloc[:20,1].values,WGAN_sorted.iloc[:20,1].values]), columns = ['Promoters_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_fasta(\"C:/Users/EGYPT_LAPTOP/Desktop/New folder/Top similiar.fasta\",Top_Similarity_seqs,'Top similiar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promoter activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(results):\n",
    "    Results = results.split('\\n')[2:len(results)]\n",
    "    Promoter_str = \"\"\n",
    "    for line in Results:\n",
    "        line_value = line.split(\"   \")\n",
    "        line_value = [x for x in line_value if x != '']\n",
    "        line_value = \",\".join(line_value) + '\\n'\n",
    "        Promoter_str +=  line_value \n",
    "    results_frame = pd.read_csv(StringIO(Promoter_str), sep=',')\n",
    "    return(results_frame.loc[:, ' Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ids, database_seqs = read_fasta_database(\"C:/Users/EGYPT_LAPTOP/Desktop/New folder/Top similiar.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Promoter_score = []\n",
    "for i in range(len(database_ids)):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.fruitfly.org/seq_tools/promoter.html\")\n",
    "    element = driver.find_element(By.NAME,\"threshold\")\n",
    "    element.clear()\n",
    "    element.send_keys(0.85)\n",
    "    SeqArea = driver.find_element(By.NAME, \"text\")\n",
    "    SeqArea.send_keys(database_seqs[i])\n",
    "    submit = driver.find_element(By.XPATH,\"/html/body/div[2]/div[3]/table/tbody/tr/td/form/input[6]\").click()\n",
    "    Output = driver.find_element(By.XPATH,'/html/body').text\n",
    "    Promoter_score.append(parse_results(Output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Promoter_Results = pd.DataFrame(\n",
    "    {'Promoter_id': Top_Similarity_ids['Promoters_ids'],\n",
    "     'Promoter_score': Promoter_score,\n",
    "    })\n",
    "Promoter_Results.dropna(axis='index',inplace=True)\n",
    "Promoter_Results.to_csv('C:/Users/EGYPT_LAPTOP/Desktop/New folder/Evaluation results.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c6893f014dd9c11d769c37389f836e806679b4b88546ef9c1c7dddbebefcff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
